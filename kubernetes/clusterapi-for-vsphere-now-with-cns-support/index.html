<!doctype html><html lang=en dir=auto>
<head><meta charset=utf-8>
<meta http-equiv=x-ua-compatible content="IE=edge">
<meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no">
<meta name=robots content="index, follow">
<title>ClusterAPI for vSphere, now with CNS support | Blah, Cloud</title>
<meta name=keywords content="clusterapi,clusterapi vsphere,cloud native storage,kubernetes,vsphere">
<meta name=description content="Using CAPV to deploy K8s clusters with vSphere CNS">
<meta name=author content="Myles Gray">
<link rel=canonical href=https://blah.cloud/kubernetes/clusterapi-for-vsphere-now-with-cns-support/>
<link crossorigin=anonymous href=/assets/css/stylesheet.min.e38a41c5e7d4d3ea02d394ee536b3395f1cb27bd5a9162de1e375c7d2d1bc8f0.css integrity="sha256-44pBxefU0+oC05TuU2szlfHLJ71akWLeHjdcfS0byPA=" rel="preload stylesheet" as=style>
<link rel=preload href=/images/logo-title.png as=image>
<script defer crossorigin=anonymous src=/assets/js/scroll-toc.min.c4f9a8e78694df8c52f7d3b0c44492ff3dcfa95d42215176796bef76438bc463.js integrity="sha256-xPmo54aU34xS99OwxESS/z3PqV1CIVF2eWvvdkOLxGM="></script>
<link rel=icon href=https://blah.cloud/images/favicon.ico>
<link rel=icon type=image/png sizes=16x16 href=https://blah.cloud/images/favicon-16x16.png>
<link rel=icon type=image/png sizes=32x32 href=https://blah.cloud/images/favicon-32x32.png>
<link rel=apple-touch-icon href=https://blah.cloud/images/apple-touch-icon.png>
<link rel=mask-icon href=https://blah.cloud/images/logo.png>
<meta name=theme-color content="#2e2e33">
<meta name=msapplication-TileColor content="#2e2e33">
<meta name=generator content="Hugo 0.88.1">
<meta property="og:title" content="ClusterAPI for vSphere, now with CNS support">
<meta property="og:description" content="Using CAPV to deploy K8s clusters with vSphere CNS">
<meta property="og:type" content="article">
<meta property="og:url" content="https://blah.cloud/kubernetes/clusterapi-for-vsphere-now-with-cns-support/">
<meta property="og:image" content="https://blah.cloud/images/cns-vols.png"><meta property="article:section" content="posts">
<meta property="article:published_time" content="2019-10-10T12:21:35+00:00">
<meta property="article:modified_time" content="2021-10-25T15:19:00+00:00"><meta property="og:site_name" content="Blah, Cloud">
<meta property="og:see_also" content="https://blah.cloud/automation/using-velero-for-k8s-backup-and-restore-of-csi-volumes/"><meta property="og:see_also" content="https://blah.cloud/kubernetes/first-look-automated-k8s-lifecycle-with-clusterapi/"><meta property="og:see_also" content="https://blah.cloud/infrastructure/using-cloud-init-for-vm-templating-on-vsphere/"><meta property="og:see_also" content="https://blah.cloud/kubernetes/using-the-vsphere-cloud-provider-for-k8s-to-dynamically-deploy-volumes/"><meta property="og:see_also" content="https://blah.cloud/kubernetes/setting-up-k8s-and-the-vsphere-cloud-provider-using-kubeadm/">
<meta name=twitter:card content="summary_large_image">
<meta name=twitter:image content="https://blah.cloud/images/cns-vols.png">
<meta name=twitter:title content="ClusterAPI for vSphere, now with CNS support">
<meta name=twitter:description content="Using CAPV to deploy K8s clusters with vSphere CNS">
<script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":2,"name":"ClusterAPI for vSphere, now with CNS support","item":"https://blah.cloud/kubernetes/clusterapi-for-vsphere-now-with-cns-support/"}]}</script>
<script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"ClusterAPI for vSphere, now with CNS support","name":"ClusterAPI for vSphere, now with CNS support","description":"Using CAPV to deploy K8s clusters with vSphere CNS","keywords":["clusterapi","clusterapi vsphere","cloud native storage","kubernetes","vsphere"],"articleBody":"Introduction If you want to learn about the basics and key concepts of ClusterAPI, then check out my post on the Alpha back in June here - it covers the high level concepts and troubleshooting of ClusterAPI, as well as what it offers to you as a user who wants to set up Kubernetes.\nThis blog is a look at what has changed and how you can use ClusterAPI to deploy K8s clusters on vSphere that use CNS and the CSI plugin for storage, that was introduced as part of vSphere 6.7 U3. If you want a video overview of CNS and CSI, check out my YouTube video here.\nPrerequisites Tools I am using macOS, so will be using the brew package manager to install and manage my tools, if you are using Linux or Windows, use the appropriate install guide for each tool, according to your OS.\nFor each tool I will list the brew install command and the link to the install instructions for other OSes.\n brew  https://brew.sh   git - brew install git  https://git-scm.com   go - brew install go  https://golang.org   govc - brew tap govmomi/tap/govc \u0026\u0026 brew install govmomi/tap/govc  https://github.com/vmware/govmomi/tree/master/govc   kubectl - brew install kubernetes-cli  https://kubernetes.io/docs/tasks/tools/install-kubectl/   kind (Kubernetes-in-Docker) - No brew installer yet  https://github.com/kubernetes-sigs/kind   clusterctl  https://github.com/kubernetes-sigs/cluster-api/releases    Clusterctl installation clusterctl is built currently as part of ClusterAPI upstream, so can be downloaded from there:\ncurl -Lo ./clusterctl-darwin-amd64 https://github.com/kubernetes-sigs/cluster-api/releases/download/v0.2.4/clusterctl-darwin-amd64 chmod +x ./clusterctl-darwin-amd64 mv clusterctl-darwin-amd64 /usr/local/bin/clusterctl Kind installation kind hasn’t been bundled into brew, yet - so we need to install it the old-fashioned way (this is for macOS, as an example):\ncurl -Lo ./kind-darwin-amd64 https://github.com/kubernetes-sigs/kind/releases/download/v0.5.1/kind-darwin-amd64 chmod +x ./kind-darwin-amd64 mv ./kind-darwin-amd64 /usr/local/bin/kind Environment Setup The first thing we need is to ensure your vSphere environment is on 6.7 U3, CSI depends on the CNS API in vCenter, which is only present in 6.7 U3 and higher.\nPull down the OS image The next thing we need to do is pull down the guest OS image that will be deployed to build our K8s cluster. The CAPV team have built a number of images for K8s that you can choose from here.\nA point of note - if you are using 6.7U3 and wish to use CSI/CNS - then you need to ensure the VMHW (aka the VMX version) is at 13 or higher, this is done by default on images in the table on the above link that are on K8s v1.15.4 and above, so it is recommended you use one of those. If not, you can always upgrade the template post deploy as in the getting started guide.\nThe images come in two flavours currently, CentOS and Ubuntu, i’m downloading and using an Ubuntu 18.04 version with K8s v1.15.4:\nwget https://storage.googleapis.com/capv-images/release/v1.15.4/ubuntu-1804-kube-v1.15.4.ova -P ~/Downloads/ Set up vSphere with govc Fill in the appropriate environment variables for your vSphere environment to allow us to connect with govc (I put this in a file called govcvars.sh):\nexport GOVC_INSECURE=1 export GOVC_URL=vc01.satm.eng.vmware.com export GOVC_USERNAME=administrator@vsphere.local export GOVC_PASSWORD=P@ssw0rd export GOVC_DATASTORE=vsanDatastore export GOVC_NETWORK=\"Cluster01-LAN-1-Routable\" export GOVC_RESOURCE_POOL='cluster01/Resources' export GOVC_DATACENTER=DC01 Import the env vars into our shell and connect to the vCenter with govc:\nsource govcvars.sh govc about Now that we’re connected to vCenter, let’s create some folders for our templates and cluster VMs to live in:\ngovc folder.create /$GOVC_DATACENTER/vm/Templates govc folder.create /$GOVC_DATACENTER/vm/Testing govc folder.create /$GOVC_DATACENTER/vm/Testing/K8s Customise and import the template VM There have been some changes to the OVA and OVF image building process, so if you followed along last time - this is slightly different now. Let’s extract the OVF spec from the template and change the Network to the name of your Port Group in vSphere and MarkAsTemplate to true as that’s what it’s going to end up as anyway - may as well do it on import!\nBecause we left the Name parameter as null, it will automatically be named ubuntu-1804-kube-v1.15.4) and we will use that name for the rest of this blog, so if you changed Name keep and eye out and change those as we go along.\ngovc import.spec ~/Downloads/ubuntu-1804-kube-v1.15.4.ova | python -m json.tool  ubuntu.json Edit the ubuntu.json to reflect your preferences:\n{ \"DiskProvisioning\": \"thin\", \"IPAllocationPolicy\": \"dhcpPolicy\", \"IPProtocol\": \"IPv4\", \"NetworkMapping\": [ { \"Name\": \"nic0\", \"Network\": \"Cluster01-LAN-1-Routable\" } ], \"Annotation\": \"Cluster API vSphere image - Ubuntu 18.04 and Kubernetes v1.15.4 - https://github.com/kubernetes-sigs/cluster-api-provider-vsphere/tree/master/build/images\", \"MarkAsTemplate\": true, \"PowerOn\": false, \"InjectOvfEnv\": false, \"WaitForIP\": false, \"Name\": null } Let’s import the template we just downloaded into VC and the folder that we just created:\ngovc import.ova -folder /$GOVC_DATACENTER/vm/Templates -options ubuntu.json ~/Downloads/ubuntu-1804-kube-v1.15.4.ova Using ClusterAPI During the Alpha, we had to build clusterctl from source - no longer! If you followed the instructions above, you should have clusterctl available in your PATH so the following command should show you the help output:\nclusterctl -h Management Cluster Define your K8s Cluster Specification Creating cluster manifests has also changed and is much simpler now, all built into a Docker container for us to use.\nSo, let’s define where our cluster should be deployed, the name of it, K8s version, what SSH keys should be added to the guest’s trusted store and how many resources it should have by filling in the following environment variables (I put the below in a file called envvars.txt) - change the below to suit your environment:\ncat envvars.txt # K8s attributes export KUBERNETES_VERSION='1.15.4' # vSphere attributes export VSPHERE_USERNAME=administrator@vsphere.local export VSPHERE_PASSWORD=P@ssw0rd export VSPHERE_SERVER=vc01.satm.eng.vmware.com # vSphere deployment configs export VSPHERE_DATACENTER=DC01 export VSPHERE_DATASTORE=vsanDatastore export VSPHERE_NETWORK=\"Cluster01-LAN-1-Routable\" export VSPHERE_RESOURCE_POOL=\"cluster01/Resources/CAPV\" export VSPHERE_FOLDER=\"/DC01/vm/Testing/K8s\" export VSPHERE_TEMPLATE=\"ubuntu-1804-kube-v1.15.4\" export VSPHERE_DISK_GIB=60 export VSPHERE_NUM_CPUS=\"2\" export VSPHERE_MEM_MIB=\"2048\" export SSH_AUTHORIZED_KEY='ssh-rsa AAAAB3......w== myles@vmware.com' EOF Let’s create the manifest files that will define and create our cluster when we plug them into clusterctl:\ndocker run --rm \\  -v \"$(pwd)\":/out \\  -v \"$(pwd)/envvars.txt\":/envvars.txt:ro \\  gcr.io/cluster-api-provider-vsphere/release/manifests:latest \\  -c management-cluster This has placed the yaml files in a new directory ./out, so let’s use clusterctl to spin up a brand new management K8s cluster:\nCreate the Management Cluster The below command plugs in the manifest files created above in order to define our CAPV management cluster - you can change the yaml files from above to suit your liking, or change things in the command like the name of the cluster.\nclusterctl create cluster \\  --bootstrap-type kind \\  --bootstrap-flags name=capv-cluster-mgmt-01 \\  --cluster ./out/management-cluster/cluster.yaml \\  --machines ./out/management-cluster/controlplane.yaml \\  --provider-components ./out/management-cluster/provider-components.yaml \\  --addon-components ./out/management-cluster/addons.yaml \\  --kubeconfig-out ./out/management-cluster/kubeconfig This will take in the order of 5-10 minutes depending on your environment, it will create a kind single node K8s cluster on your local machine within a Docker container to act as a bootstrap.\nIt then creates another single-node K8s VM on your target vSphere environment with the same configuration, and deletes the kind cluster from your local machine, because it was only there to act as a bootstrap.\nAt this point, clusterctl will spit out the kubeconfig for your management cluster into the ./out/management-cluster/kubeconfig directory and you should be able to connect to your ClusterAPI “management” cluster:\nExport the newly downloaded kubeconfig file so it’s the default for kubectl to use:\nexport KUBECONFIG=./out/management-cluster/kubeconfig Check to see that the ClusterAPI items have been created (i.e. one cluster and one machine) for the management cluster.\nkubectl get clusters kubectl get machines Workload Clusters Define your Workload Cluster Specification With the ClusterAPI management cluster deployed, we can now use it, along with kubectl to create other K8s workload clusters!\nThe workload clusters use much the same process as the management cluster, however it makes sense to look into the yaml files generated in order to do things like define the number of worker nodes and such.\nThis time we’ll create some more manifests for our workload cluster (note the name workload-cluster-01):\ndocker run --rm \\  -v \"$(pwd)\":/out \\  -v \"$(pwd)/envvars.txt\":/envvars.txt:ro \\  gcr.io/cluster-api-provider-vsphere/release/manifests:latest \\  -c workload-cluster-01 If you check out the yaml files that were generated, in particular check out the machinedeployment.yaml file and adjust the number of replicas in the MachineDeployment section as below - this would give you 3 worker nodes in your cluster, instead of the default 1:\napiVersion:cluster.x-k8s.io/v1alpha2kind:MachineDeploymentmetadata:labels:cluster.x-k8s.io/cluster-name:workload-cluster-01name:workload-cluster-01-md-0namespace:defaultspec:replicas:3Create the Workload Cluster Let’s use the ClusterAPI management cluster to create our new workload cluster - we do this by passing in the yaml that was just generated to the management cluster, the ClusterAPI controller within the management cluster will then look at the specifications for each and create a new K8s cluster with the number of masters and workers as defined in controlplane.yaml and machinedeployment.yaml files respectively.\nFirst, let’s export KUBECONFIG so we are interacting with the management cluster:\nexport KUBECONFIG=./out/management-cluster/kubeconfig Next, let’s import the yaml files that define the workload cluster:\nkubectl apply -f ./out/workload-cluster-01/cluster.yaml -f ./out/workload-cluster-01/controlplane.yaml -f ./out/workload-cluster-01/machinedeployment.yaml If we watch ClusterAPI’s machines CRD we can see that it will have created a master and three workers if you changed the yaml to my change as above. This will take a few minutes, so it’s best to run this command and wait until all machines show Running.\nkubectl get machines -w Once all the machines are Running we will be able to pull down the kubeconfig for that cluster so we can deploy workloads on to it.\nConnecting to the Workload Cluster We’ve successfully provisioned our workload cluster, but how do we access and use it?\nGood question, when using ClusterAPI to spin up workload clusters, it needs to put the access credentials (i.e. the kubeconfig file) somewhere, so it puts them in a K8s secret, luckily they are very easy to retrieve and decode to your local machine.\nkubectl get secret workload-cluster-01-kubeconfig -o=jsonpath='{.data.value}' | { base64 -d 2/dev/null || base64 -D; } ./out/workload-cluster-01/kubeconfig Notice the workload-cluster-01-kubeconfig secret - this is what we want to connect to our workload cluster, it’s very easy to extract and pull this to your local machine. The command pulls the secret value which is base64 encoded in K8s - decodes it from base64 to text and creates a new kubeconfig file in the workload cluster’s directory on your laptop.\nLet’s apply the addons to our workload cluster (these are mainly just the networking overlay, Calico) - required to let pods talk to one-another - we will first change clusters by exporting KUBECONFIG once again:\nexport KUBECONFIG=./out/workload-cluster-01/kubeconfig kubectl apply -f ./out/workload-cluster-01/addons.yaml And watch as the pods get spun up, when it’s all working - everything should list as Running:.\nkubectl get pods -n kube-system -w Deploy some applications Now that the workload cluster is set up, we can deploy some apps to it - because ClusterAPI also takes care of the CSI setup, we can even deploy ones that use persistent storage!\nWe’re going to deploy use helm to set up an application called RocketChat on the cluster, which uses two persistent volumes, one for config and one for its MongoDB database.\nConfigure helm Be aware this installation style for helm (granting the tiller pod cluster-admin privileges) is a big security no-no and is just for ease of setup here. For more information on why this is bad, look here, and please don’t do this on a production cluster.\nIn this case, it is a throwaway cluster for me, so I will be using these permissions. First create the RBAC role and permissions for the helm service account in another new file called helm-rbac.yaml:\n$ cat helm-rbac.yamlapiVersion:v1kind:ServiceAccountmetadata:name:tillernamespace:kube-system---apiVersion:rbac.authorization.k8s.io/v1kind:ClusterRoleBindingmetadata:name:tillerroleRef:apiGroup:rbac.authorization.k8s.iokind:ClusterRolename:cluster-adminsubjects:- kind:ServiceAccountname:tillernamespace:kube-systemEOFApply the role to the cluster:\nkubectl apply -f helm-rbac.yaml Let’s install helm onto the cluster with the service account we provisioned:\nhelm init --service-account tiller Configure a StorageClass We’re going to delete the default StorageClass that gets deployed and instead, create our own that uses the CSI plugin that is installed by default with CAPV.\nkubectl delete sc --all Create a new StorageClass yaml that uses the CSI provisioner (and by extension, CNS) - note the provisioner line:\n$ cat sc.yamlkind:StorageClassapiVersion:storage.k8s.io/v1metadata:name:standardannotations:storageclass.kubernetes.io/is-default-class:\"true\"provisioner:csi.vsphere.vmware.comparameters:storagePolicyName:\"vSAN Default Storage Policy\"EOFThe above StorageClass uses the vSAN Default Storage Policy within vCenter, but you can change it to your own - the name of the SC in this case is standard and we’ll use it deploying a demo app next.\nkubectl apply -f sc.yaml Provision an application We’re now in a place where we can provision an application, we’re going to use helm to install RocketChat, as discussed above - RocketChat is basically an Open-Source Slack clone that you can run on-prem.\nThe below command tells helm to install RocketChat from the stable/rocketchat repository, give it a name, set the passwords for MongoDB and most critically - use the standard StorageClass that we just imported into the workload cluster to back the PersistentVolumes requested by RocketChat:\nhelm install stable/rocketchat --set persistence.StorageClass=standard,mongodb.mongodbPassword=password,mongodb.mongodbRootPassword=password Verify the volumes got provisioned (this will take a minute before it returns back the “Bound” status):\n$ kubectl get pv,pvc NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGE persistentvolume/pvc-3c754fc9-f7bb-448f-8f2c-510fedc0cebc 8Gi RWO Delete Bound default/datadir-exacerbated-squirrel-mongodb-secondary-0 standard 19h persistentvolume/pvc-47ad5e4d-4cdd-4c14-9148-5e9a2321bb8e 8Gi RWO Delete Bound default/datadir-exacerbated-squirrel-mongodb-primary-0 standard 19h NAMESPACE NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE default persistentvolumeclaim/datadir-exacerbated-squirrel-mongodb-primary-0 Bound pvc-47ad5e4d-4cdd-4c14-9148-5e9a2321bb8e 8Gi RWO standard 19h default persistentvolumeclaim/datadir-exacerbated-squirrel-mongodb-secondary-0 Bound pvc-3c754fc9-f7bb-448f-8f2c-510fedc0cebc 8Gi RWO standard 19h And the pods for the application should be running:\n$ kubectl get po NAME READY STATUS RESTARTS AGE exacerbated-squirrel-mongodb-arbiter-0 1/1 Running 0 19h exacerbated-squirrel-mongodb-primary-0 1/1 Running 0 19h exacerbated-squirrel-mongodb-secondary-0 1/1 Running 0 19h exacerbated-squirrel-rocketchat-958b577d-v8vzp 1/1 Running 0 19h At this point, we can access the application by port-forwarding to the rocketchat-rocketchat-* pod from the output above (change this to suit your pod name):\nkubectl -port-forward exacerbated-squirrel-rocketchat-958b577d-v8vzp 8888:3000 Access the application on localhost:8888 in your web browser:\nopen http://localhost:8888 In the new CNS UI within vSphere 6.7 U3 - we can see the volumes that have been deployed from this K8s cluster via the CSI plugin:\nWhy not follow @mylesagray on Twitter for more like this!\n","wordCount":"2257","inLanguage":"en","image":"https://blah.cloud/images/cns-vols.png","datePublished":"2019-10-10T12:21:35Z","dateModified":"2021-10-25T15:19:00Z","author":{"@type":"Person","name":"Myles Gray","url":"/about"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://blah.cloud/kubernetes/clusterapi-for-vsphere-now-with-cns-support/"},"publisher":{"@type":"Organization","name":"Blah, Cloud","logo":{"@type":"ImageObject","url":"https://blah.cloud/images/favicon.ico"}}}</script>
<script defer data-domain=blah.cloud src=https://plausible.io/js/plausible.js></script>
<script>window.plausible=window.plausible||function(){(window.plausible.q=window.plausible.q||[]).push(arguments)}</script>
</head>
<body id=top>
<script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add('dark'):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove('dark'):window.matchMedia('(prefers-color-scheme: dark)').matches&&document.body.classList.add('dark')</script>
<script>var themeColor=getComputedStyle(document.body).getPropertyValue('--primary');document.querySelector('meta[name="theme-color"]').setAttribute('content',themeColor)</script>
<noscript>
<style type=text/css>#theme-toggle,.top-link{display:none}</style>
<style>@media(prefers-color-scheme:dark){:root{--theme:#1d1e20;--entry:#2e2e33;--primary:rgba(240, 202, 102, 1);--secondary:rgba(216, 214, 197, 1);--tertiary:rgba(128, 130 ,133 , 1);--content:rgba(206, 205, 188, 1);--hljs-bg:#282a36;--code-bg:#282a36;--border:rgba(240, 202, 102, 1)}.list{background:var(--theme)}}</style>
</noscript>
<header class=header>
<nav class=nav>
<div class=logo>
<a href=https://blah.cloud/ accesskey=h title="Blah, Cloud. (Alt + H)">
<img src=/images/logo-title.png alt=logo aria-label=logo height=40></a>
</div>
<span class=logo-switches>
<button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg>
</button>
</span>
<ul id=menu>
<li>
<a href=https://blah.cloud/blog/ title=blog>
<span>blog</span>
</a>
</li>
<li>
<a href=https://blah.cloud/search/ title=" (Alt + /)" accesskey=/>
<span><svg style="height:1em;margin-top:1.5em" aria-hidden="true" focusable="false" data-prefix="fas" data-icon="search" class="fill-current w-5" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path d="M505 442.7 405.3 343c-4.5-4.5-10.6-7-17-7H372c27.6-35.3 44-79.7 44-128C416 93.1 322.9.0 208 0S0 93.1.0 208s93.1 208 208 208c48.3.0 92.7-16.4 128-44v16.3c0 6.4 2.5 12.5 7 17l99.7 99.7c9.4 9.4 24.6 9.4 33.9.0l28.3-28.3c9.4-9.4 9.4-24.6.1-34zM208 336c-70.7.0-128-57.2-128-128 0-70.7 57.2-128 128-128 70.7.0 128 57.2 128 128 0 70.7-57.2 128-128 128z"/></svg></span>
</a>
</li>
</ul>
</nav>
</header>
<div class=container>
<main class=main>
<div class=wrapper>
<article class=post-single>
<header class=post-header>
<h1 class=post-title>
ClusterAPI for vSphere, now with CNS support
</h1>
<div class=post-description>
Using CAPV to deploy K8s clusters with vSphere CNS
</div>
<div class=post-meta>October 10, 2019&nbsp;·&nbsp;Myles Gray&nbsp;|&nbsp;<a href=https://github.com/mylesagray/blog/blob/master/content/posts/2019-10-10-clusterapi-for-vsphere-now-with-cns-support/index.md rel="noopener noreferrer" target=_blank>Suggest Changes</a>
</div>
</header>
<figure class=entry-cover>
<img loading=lazy srcset="https://blah.cloud/kubernetes/clusterapi-for-vsphere-now-with-cns-support/images/cns-vols_hua5cca19d8d5d7fb46f0d627a827b5c3f_80364_360x0_resize_box_3.png 360w ,https://blah.cloud/kubernetes/clusterapi-for-vsphere-now-with-cns-support/images/cns-vols_hua5cca19d8d5d7fb46f0d627a827b5c3f_80364_480x0_resize_box_3.png 480w ,https://blah.cloud/kubernetes/clusterapi-for-vsphere-now-with-cns-support/images/cns-vols_hua5cca19d8d5d7fb46f0d627a827b5c3f_80364_720x0_resize_box_3.png 720w ,https://blah.cloud/kubernetes/clusterapi-for-vsphere-now-with-cns-support/images/cns-vols_hua5cca19d8d5d7fb46f0d627a827b5c3f_80364_1080x0_resize_box_3.png 1080w ,https://blah.cloud/kubernetes/clusterapi-for-vsphere-now-with-cns-support/images/cns-vols_hua5cca19d8d5d7fb46f0d627a827b5c3f_80364_1500x0_resize_box_3.png 1500w ,https://blah.cloud/kubernetes/clusterapi-for-vsphere-now-with-cns-support/images/cns-vols.png 1510w" sizes="(min-width: 768px) 720px, 100vw" src=https://blah.cloud/kubernetes/clusterapi-for-vsphere-now-with-cns-support/images/cns-vols.png alt="CNS Volume List" width=1510 height=774>
</figure><div class=toc>
<details>
<summary accesskey=c title="(Alt + C)">
<span class=details>Table of Contents</span>
</summary>
<div class=inner><ul>
<li>
<a href=#introduction aria-label=Introduction>Introduction</a></li>
<li>
<a href=#prerequisites aria-label=Prerequisites>Prerequisites</a><ul>
<li>
<a href=#tools aria-label=Tools>Tools</a><ul>
<li>
<a href=#clusterctl-installation aria-label="Clusterctl installation">Clusterctl installation</a></li>
<li>
<a href=#kind-installation aria-label="Kind installation">Kind installation</a></li></ul>
</li></ul>
</li>
<li>
<a href=#environment-setup aria-label="Environment Setup">Environment Setup</a><ul>
<li>
<a href=#pull-down-the-os-image aria-label="Pull down the OS image">Pull down the OS image</a></li>
<li>
<a href=#set-up-vsphere-with-govc aria-label="Set up vSphere with govc">Set up vSphere with govc</a></li>
<li>
<a href=#customise-and-import-the-template-vm aria-label="Customise and import the template VM">Customise and import the template VM</a></li></ul>
</li>
<li>
<a href=#using-clusterapi aria-label="Using ClusterAPI">Using ClusterAPI</a><ul>
<li>
<a href=#management-cluster aria-label="Management Cluster">Management Cluster</a><ul>
<li>
<a href=#define-your-k8s-cluster-specification aria-label="Define your K8s Cluster Specification">Define your K8s Cluster Specification</a></li>
<li>
<a href=#create-the-management-cluster aria-label="Create the Management Cluster">Create the Management Cluster</a></li></ul>
</li>
<li>
<a href=#workload-clusters aria-label="Workload Clusters">Workload Clusters</a><ul>
<li>
<a href=#define-your-workload-cluster-specification aria-label="Define your Workload Cluster Specification">Define your Workload Cluster Specification</a></li>
<li>
<a href=#create-the-workload-cluster aria-label="Create the Workload Cluster">Create the Workload Cluster</a></li></ul>
</li></ul>
</li>
<li>
<a href=#connecting-to-the-workload-cluster aria-label="Connecting to the Workload Cluster">Connecting to the Workload Cluster</a></li>
<li>
<a href=#deploy-some-applications aria-label="Deploy some applications">Deploy some applications</a><ul>
<li>
<a href=#configure-helm aria-label="Configure helm">Configure helm</a></li>
<li>
<a href=#configure-a-storageclass aria-label="Configure a StorageClass">Configure a StorageClass</a></li>
<li>
<a href=#provision-an-application aria-label="Provision an application">Provision an application</a>
</li>
</ul>
</li>
</ul>
</div>
</details>
</div>
<div class=post-content><h2 id=introduction>Introduction<a hidden class=anchor aria-hidden=true href=#introduction>#</a></h2>
<p>If you want to learn about the basics and key concepts of ClusterAPI, then check out my post on the Alpha back <a href=/kubernetes/first-look-automated-k8s-lifecycle-with-clusterapi/>in June here</a> - it covers the high level concepts and troubleshooting of ClusterAPI, as well as what it offers to you as a user who wants to set up Kubernetes.</p>
<p>This blog is a look at what has changed and how you can use ClusterAPI to deploy K8s clusters on vSphere that use CNS and the CSI plugin for storage, that was <a href=https://blogs.vmware.com/virtualblocks/2019/08/14/introducing-cloud-native-storage-for-vsphere/>introduced as part of vSphere 6.7 U3</a>. If you want a video overview of CNS and CSI, check out my <a href="https://www.youtube.com/watch?v=CZUBuWCgeDQ">YouTube video here</a>.</p>
<h2 id=prerequisites>Prerequisites<a hidden class=anchor aria-hidden=true href=#prerequisites>#</a></h2>
<h3 id=tools>Tools<a hidden class=anchor aria-hidden=true href=#tools>#</a></h3>
<p>I am using macOS, so will be using the <code>brew</code> package manager to install and manage my tools, if you are using Linux or Windows, use the appropriate install guide for each tool, according to your OS.</p>
<p>For each tool I will list the <code>brew</code> install command and the link to the install instructions for other OSes.</p>
<ul>
<li>brew
<ul>
<li><a href=https://brew.sh>https://brew.sh</a></li>
</ul>
</li>
<li>git - <code>brew install git</code>
<ul>
<li><a href=https://git-scm.com>https://git-scm.com</a></li>
</ul>
</li>
<li>go - <code>brew install go</code>
<ul>
<li><a href=https://golang.org>https://golang.org</a></li>
</ul>
</li>
<li>govc - <code>brew tap govmomi/tap/govc && brew install govmomi/tap/govc</code>
<ul>
<li><a href=https://github.com/vmware/govmomi/tree/master/govc>https://github.com/vmware/govmomi/tree/master/govc</a></li>
</ul>
</li>
<li>kubectl - <code>brew install kubernetes-cli</code>
<ul>
<li><a href=https://kubernetes.io/docs/tasks/tools/install-kubectl/>https://kubernetes.io/docs/tasks/tools/install-kubectl/</a></li>
</ul>
</li>
<li>kind (Kubernetes-in-Docker) - No brew installer yet
<ul>
<li><a href=https://github.com/kubernetes-sigs/kind>https://github.com/kubernetes-sigs/kind</a></li>
</ul>
</li>
<li>clusterctl
<ul>
<li><a href=https://github.com/kubernetes-sigs/cluster-api/releases>https://github.com/kubernetes-sigs/cluster-api/releases</a></li>
</ul>
</li>
</ul>
<h4 id=clusterctl-installation>Clusterctl installation<a hidden class=anchor aria-hidden=true href=#clusterctl-installation>#</a></h4>
<p><code>clusterctl</code> is built currently as part of ClusterAPI upstream, so can be downloaded <a href=https://github.com/kubernetes-sigs/cluster-api/releases>from there</a>:</p>
<div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash>curl -Lo ./clusterctl-darwin-amd64 https://github.com/kubernetes-sigs/cluster-api/releases/download/v0.2.4/clusterctl-darwin-amd64
chmod +x ./clusterctl-darwin-amd64
mv clusterctl-darwin-amd64 /usr/local/bin/clusterctl
</code></pre></div><h4 id=kind-installation>Kind installation<a hidden class=anchor aria-hidden=true href=#kind-installation>#</a></h4>
<p><code>kind</code> hasn&rsquo;t been bundled into <code>brew</code>, yet - so we need to install it the old-fashioned way (this is for macOS, as an example):</p>
<div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash>curl -Lo ./kind-darwin-amd64 https://github.com/kubernetes-sigs/kind/releases/download/v0.5.1/kind-darwin-amd64
chmod +x ./kind-darwin-amd64
mv ./kind-darwin-amd64 /usr/local/bin/kind
</code></pre></div><h2 id=environment-setup>Environment Setup<a hidden class=anchor aria-hidden=true href=#environment-setup>#</a></h2>
<p>The first thing we need is to ensure your vSphere environment is on 6.7 U3, CSI depends on the CNS API in vCenter, which is only present in 6.7 U3 and higher.</p>
<h3 id=pull-down-the-os-image>Pull down the OS image<a hidden class=anchor aria-hidden=true href=#pull-down-the-os-image>#</a></h3>
<p>The next thing we need to do is pull down the guest OS image that will be deployed to build our K8s cluster. The CAPV team have built a number of images for K8s that <a href=https://github.com/kubernetes-sigs/cluster-api-provider-vsphere#kubernetes-versions-with-published-ovas>you can choose from here</a>.</p>
<p>A point of note - if you are using 6.7U3 and wish to use CSI/CNS - then you need to ensure the <code>VMHW</code> (aka the <code>VMX</code> version) is at <code>13</code> or higher, this is done by default on images in the table on the above link that are on K8s <code>v1.15.4</code> and above, so it is recommended you use one of those. If not, you can always upgrade the template post deploy as in the <a href=https://github.com/kubernetes-sigs/cluster-api-provider-vsphere/blob/master/docs/getting_started.md#uploading-the-capv-machine-image>getting started guide</a>.</p>
<p>The images come in two flavours currently, CentOS and Ubuntu, i&rsquo;m downloading and using an Ubuntu 18.04 version with K8s v1.15.4:</p>
<div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash>wget https://storage.googleapis.com/capv-images/release/v1.15.4/ubuntu-1804-kube-v1.15.4.ova -P ~/Downloads/
</code></pre></div><h3 id=set-up-vsphere-with-govc>Set up vSphere with govc<a hidden class=anchor aria-hidden=true href=#set-up-vsphere-with-govc>#</a></h3>
<p>Fill in the appropriate environment variables for your vSphere environment to allow us to connect with <code>govc</code> (I put this in a file called <code>govcvars.sh</code>):</p>
<div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=nb>export</span> <span class=nv>GOVC_INSECURE</span><span class=o>=</span><span class=m>1</span>
<span class=nb>export</span> <span class=nv>GOVC_URL</span><span class=o>=</span>vc01.satm.eng.vmware.com
<span class=nb>export</span> <span class=nv>GOVC_USERNAME</span><span class=o>=</span>administrator@vsphere.local
<span class=nb>export</span> <span class=nv>GOVC_PASSWORD</span><span class=o>=</span>P@ssw0rd
<span class=nb>export</span> <span class=nv>GOVC_DATASTORE</span><span class=o>=</span>vsanDatastore
<span class=nb>export</span> <span class=nv>GOVC_NETWORK</span><span class=o>=</span><span class=s2>&#34;Cluster01-LAN-1-Routable&#34;</span>
<span class=nb>export</span> <span class=nv>GOVC_RESOURCE_POOL</span><span class=o>=</span><span class=s1>&#39;cluster01/Resources&#39;</span>
<span class=nb>export</span> <span class=nv>GOVC_DATACENTER</span><span class=o>=</span>DC01
</code></pre></div><p>Import the env vars into our shell and connect to the vCenter with <code>govc</code>:</p>
<div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=nb>source</span> govcvars.sh
govc about
</code></pre></div><p>Now that we&rsquo;re connected to vCenter, let&rsquo;s create some folders for our templates and cluster VMs to live in:</p>
<div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash>govc folder.create /<span class=nv>$GOVC_DATACENTER</span>/vm/Templates
govc folder.create /<span class=nv>$GOVC_DATACENTER</span>/vm/Testing
govc folder.create /<span class=nv>$GOVC_DATACENTER</span>/vm/Testing/K8s
</code></pre></div><h3 id=customise-and-import-the-template-vm>Customise and import the template VM<a hidden class=anchor aria-hidden=true href=#customise-and-import-the-template-vm>#</a></h3>
<p>There have been some changes to the OVA and OVF image building process, so if you followed along last time - this is slightly different now. Let&rsquo;s extract the OVF spec from the template and change the <code>Network</code> to the name of your Port Group in vSphere and <code>MarkAsTemplate</code> to <code>true</code> as that&rsquo;s what it&rsquo;s going to end up as anyway - may as well do it on import!</p>
<p>Because we left the <code>Name</code> parameter as null, it will automatically be named <code>ubuntu-1804-kube-v1.15.4</code>) and we will use that name for the rest of this blog, so if you changed <code>Name</code> keep and eye out and change those as we go along.</p>
<div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash>govc import.spec ~/Downloads/ubuntu-1804-kube-v1.15.4.ova <span class=p>|</span> python -m json.tool &gt; ubuntu.json
</code></pre></div><p>Edit the <code>ubuntu.json</code> to reflect your preferences:</p>
<div class=highlight><pre tabindex=0 class=chroma><code class=language-json data-lang=json><span class=p>{</span>
    <span class=nt>&#34;DiskProvisioning&#34;</span><span class=p>:</span> <span class=s2>&#34;thin&#34;</span><span class=p>,</span>
    <span class=nt>&#34;IPAllocationPolicy&#34;</span><span class=p>:</span> <span class=s2>&#34;dhcpPolicy&#34;</span><span class=p>,</span>
    <span class=nt>&#34;IPProtocol&#34;</span><span class=p>:</span> <span class=s2>&#34;IPv4&#34;</span><span class=p>,</span>
    <span class=nt>&#34;NetworkMapping&#34;</span><span class=p>:</span> <span class=p>[</span>
        <span class=p>{</span>
            <span class=nt>&#34;Name&#34;</span><span class=p>:</span> <span class=s2>&#34;nic0&#34;</span><span class=p>,</span>
            <span class=nt>&#34;Network&#34;</span><span class=p>:</span> <span class=s2>&#34;Cluster01-LAN-1-Routable&#34;</span>
        <span class=p>}</span>
    <span class=p>],</span>
    <span class=nt>&#34;Annotation&#34;</span><span class=p>:</span> <span class=s2>&#34;Cluster API vSphere image - Ubuntu 18.04 and Kubernetes v1.15.4 - https://github.com/kubernetes-sigs/cluster-api-provider-vsphere/tree/master/build/images&#34;</span><span class=p>,</span>
    <span class=nt>&#34;MarkAsTemplate&#34;</span><span class=p>:</span> <span class=kc>true</span><span class=p>,</span>
    <span class=nt>&#34;PowerOn&#34;</span><span class=p>:</span> <span class=kc>false</span><span class=p>,</span>
    <span class=nt>&#34;InjectOvfEnv&#34;</span><span class=p>:</span> <span class=kc>false</span><span class=p>,</span>
    <span class=nt>&#34;WaitForIP&#34;</span><span class=p>:</span> <span class=kc>false</span><span class=p>,</span>
    <span class=nt>&#34;Name&#34;</span><span class=p>:</span> <span class=kc>null</span>
<span class=p>}</span>
</code></pre></div><p>Let&rsquo;s import the template we just downloaded into VC and the folder that we just created:</p>
<div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash>govc import.ova -folder /<span class=nv>$GOVC_DATACENTER</span>/vm/Templates -options ubuntu.json ~/Downloads/ubuntu-1804-kube-v1.15.4.ova
</code></pre></div><h2 id=using-clusterapi>Using ClusterAPI<a hidden class=anchor aria-hidden=true href=#using-clusterapi>#</a></h2>
<p>During the Alpha, we had to build <code>clusterctl</code> from source - no longer! If you followed the instructions above, you should have <code>clusterctl</code> available in your <code>PATH</code> so the following command should show you the help output:</p>
<div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash>clusterctl -h
</code></pre></div><h3 id=management-cluster>Management Cluster<a hidden class=anchor aria-hidden=true href=#management-cluster>#</a></h3>
<h4 id=define-your-k8s-cluster-specification>Define your K8s Cluster Specification<a hidden class=anchor aria-hidden=true href=#define-your-k8s-cluster-specification>#</a></h4>
<p>Creating cluster manifests has also changed and is much simpler now, all built into a Docker container for us to use.</p>
<p>So, let&rsquo;s define where our cluster should be deployed, the name of it, K8s version, what SSH keys should be added to the guest&rsquo;s trusted store and how many resources it should have by filling in the following environment variables (I put the below in a file called <code>envvars.txt</code>) - change the below to suit your environment:</p>
<div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash>cat <span class=s>&lt;&lt;EOF &gt;envvars.txt
</span><span class=s># K8s attributes
</span><span class=s>export KUBERNETES_VERSION=&#39;1.15.4&#39;
</span><span class=s>
</span><span class=s># vSphere attributes
</span><span class=s>export VSPHERE_USERNAME=administrator@vsphere.local
</span><span class=s>export VSPHERE_PASSWORD=P@ssw0rd
</span><span class=s>export VSPHERE_SERVER=vc01.satm.eng.vmware.com
</span><span class=s>
</span><span class=s># vSphere deployment configs
</span><span class=s>export VSPHERE_DATACENTER=DC01
</span><span class=s>export VSPHERE_DATASTORE=vsanDatastore
</span><span class=s>export VSPHERE_NETWORK=&#34;Cluster01-LAN-1-Routable&#34;
</span><span class=s>export VSPHERE_RESOURCE_POOL=&#34;cluster01/Resources/CAPV&#34;
</span><span class=s>export VSPHERE_FOLDER=&#34;/DC01/vm/Testing/K8s&#34;
</span><span class=s>export VSPHERE_TEMPLATE=&#34;ubuntu-1804-kube-v1.15.4&#34;
</span><span class=s>export VSPHERE_DISK_GIB=60
</span><span class=s>export VSPHERE_NUM_CPUS=&#34;2&#34;
</span><span class=s>export VSPHERE_MEM_MIB=&#34;2048&#34;
</span><span class=s>export SSH_AUTHORIZED_KEY=&#39;ssh-rsa AAAAB3......w== myles@vmware.com&#39;
</span><span class=s>EOF</span>
</code></pre></div><p>Let&rsquo;s create the manifest files that will define and create our cluster when we plug them into <code>clusterctl</code>:</p>
<div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash>docker run --rm <span class=se>\
</span><span class=se></span>  -v <span class=s2>&#34;</span><span class=k>$(</span><span class=nb>pwd</span><span class=k>)</span><span class=s2>&#34;</span>:/out <span class=se>\
</span><span class=se></span>  -v <span class=s2>&#34;</span><span class=k>$(</span><span class=nb>pwd</span><span class=k>)</span><span class=s2>/envvars.txt&#34;</span>:/envvars.txt:ro <span class=se>\
</span><span class=se></span>  gcr.io/cluster-api-provider-vsphere/release/manifests:latest <span class=se>\
</span><span class=se></span>  -c management-cluster
</code></pre></div><p>This has placed the <code>yaml</code> files in a new directory <code>./out</code>, so let&rsquo;s use <code>clusterctl</code> to spin up a brand new management K8s cluster:</p>
<h4 id=create-the-management-cluster>Create the Management Cluster<a hidden class=anchor aria-hidden=true href=#create-the-management-cluster>#</a></h4>
<p>The below command plugs in the manifest files created above in order to define our CAPV management cluster - you can change the <code>yaml</code> files from above to suit your liking, or change things in the command like the <code>name</code> of the cluster.</p>
<div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash>clusterctl create cluster <span class=se>\
</span><span class=se></span>  --bootstrap-type kind <span class=se>\
</span><span class=se></span>  --bootstrap-flags <span class=nv>name</span><span class=o>=</span>capv-cluster-mgmt-01 <span class=se>\
</span><span class=se></span>  --cluster ./out/management-cluster/cluster.yaml <span class=se>\
</span><span class=se></span>  --machines ./out/management-cluster/controlplane.yaml <span class=se>\
</span><span class=se></span>  --provider-components ./out/management-cluster/provider-components.yaml <span class=se>\
</span><span class=se></span>  --addon-components ./out/management-cluster/addons.yaml <span class=se>\
</span><span class=se></span>  --kubeconfig-out ./out/management-cluster/kubeconfig
</code></pre></div><p>This will take in the order of 5-10 minutes depending on your environment, it will create a <code>kind</code> single node K8s cluster on your local machine within a Docker container to act as a bootstrap.</p>
<p>It then creates another single-node K8s VM on your target vSphere environment with the same configuration, and deletes the <code>kind</code> cluster from your local machine, because it was only there to act as a bootstrap.</p>
<p>At this point, <code>clusterctl</code> will spit out the <code>kubeconfig</code> for your management cluster into the <code>./out/management-cluster/kubeconfig</code> directory and you should be able to connect to your ClusterAPI &ldquo;management&rdquo; cluster:</p>
<p>Export the newly downloaded <code>kubeconfig</code> file so it&rsquo;s the default for <code>kubectl</code> to use:</p>
<div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=nb>export</span> <span class=nv>KUBECONFIG</span><span class=o>=</span>./out/management-cluster/kubeconfig
</code></pre></div><p>Check to see that the ClusterAPI items have been created (i.e. one cluster and one machine) for the management cluster.</p>
<div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash>kubectl get clusters
kubectl get machines
</code></pre></div><h3 id=workload-clusters>Workload Clusters<a hidden class=anchor aria-hidden=true href=#workload-clusters>#</a></h3>
<h4 id=define-your-workload-cluster-specification>Define your Workload Cluster Specification<a hidden class=anchor aria-hidden=true href=#define-your-workload-cluster-specification>#</a></h4>
<p>With the ClusterAPI management cluster deployed, we can now use it, along with <code>kubectl</code> to create other K8s workload clusters!</p>
<p>The workload clusters use much the same process as the management cluster, however it makes sense to look into the <code>yaml</code> files generated in order to do things like define the number of worker nodes and such.</p>
<p>This time we&rsquo;ll create some more manifests for our workload cluster (note the name <code>workload-cluster-01</code>):</p>
<div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash>docker run --rm <span class=se>\
</span><span class=se></span>  -v <span class=s2>&#34;</span><span class=k>$(</span><span class=nb>pwd</span><span class=k>)</span><span class=s2>&#34;</span>:/out <span class=se>\
</span><span class=se></span>  -v <span class=s2>&#34;</span><span class=k>$(</span><span class=nb>pwd</span><span class=k>)</span><span class=s2>/envvars.txt&#34;</span>:/envvars.txt:ro <span class=se>\
</span><span class=se></span>  gcr.io/cluster-api-provider-vsphere/release/manifests:latest <span class=se>\
</span><span class=se></span>  -c workload-cluster-01
</code></pre></div><p>If you check out the <code>yaml</code> files that were generated, in particular check out the <code>machinedeployment.yaml</code> file and adjust the number of <code>replicas</code> in the <code>MachineDeployment</code> section as below - this would give you 3 worker nodes in your cluster, instead of the default 1:</p>
<div class=highlight><pre tabindex=0 class=chroma><code class=language-yaml data-lang=yaml><span class=nt>apiVersion</span><span class=p>:</span><span class=w> </span><span class=l>cluster.x-k8s.io/v1alpha2</span><span class=w>
</span><span class=w></span><span class=nt>kind</span><span class=p>:</span><span class=w> </span><span class=l>MachineDeployment</span><span class=w>
</span><span class=w></span><span class=nt>metadata</span><span class=p>:</span><span class=w>
</span><span class=w>  </span><span class=nt>labels</span><span class=p>:</span><span class=w>
</span><span class=w>    </span><span class=nt>cluster.x-k8s.io/cluster-name</span><span class=p>:</span><span class=w> </span><span class=l>workload-cluster-01</span><span class=w>
</span><span class=w>  </span><span class=nt>name</span><span class=p>:</span><span class=w> </span><span class=l>workload-cluster-01-md-0</span><span class=w>
</span><span class=w>  </span><span class=nt>namespace</span><span class=p>:</span><span class=w> </span><span class=l>default</span><span class=w>
</span><span class=w></span><span class=nt>spec</span><span class=p>:</span><span class=w>
</span><span class=w>  </span><span class=nt>replicas</span><span class=p>:</span><span class=w> </span><span class=m>3</span><span class=w>
</span></code></pre></div><h4 id=create-the-workload-cluster>Create the Workload Cluster<a hidden class=anchor aria-hidden=true href=#create-the-workload-cluster>#</a></h4>
<p>Let&rsquo;s use the ClusterAPI management cluster to create our new workload cluster - we do this by passing in the <code>yaml</code> that was just generated to the management cluster, the ClusterAPI controller within the management cluster will then look at the specifications for each and create a new K8s cluster with the number of masters and workers as defined in <code>controlplane.yaml</code> and <code>machinedeployment.yaml</code> files respectively.</p>
<p>First, let&rsquo;s export <code>KUBECONFIG</code> so we are interacting with the management cluster:</p>
<div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=nb>export</span> <span class=nv>KUBECONFIG</span><span class=o>=</span>./out/management-cluster/kubeconfig
</code></pre></div><p>Next, let&rsquo;s import the <code>yaml</code> files that define the workload cluster:</p>
<div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash>kubectl apply -f ./out/workload-cluster-01/cluster.yaml -f ./out/workload-cluster-01/controlplane.yaml -f ./out/workload-cluster-01/machinedeployment.yaml
</code></pre></div><p>If we watch ClusterAPI&rsquo;s machines CRD we can see that it will have created a master and three workers if you changed the <code>yaml</code> to my change as above. This will take a few minutes, so it&rsquo;s best to run this command and wait until all machines show <code>Running</code>.</p>
<div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash>kubectl get machines -w
</code></pre></div><p>Once all the machines are <code>Running</code> we will be able to pull down the <code>kubeconfig</code> for that cluster so we can deploy workloads on to it.</p>
<h2 id=connecting-to-the-workload-cluster>Connecting to the Workload Cluster<a hidden class=anchor aria-hidden=true href=#connecting-to-the-workload-cluster>#</a></h2>
<p>We&rsquo;ve successfully provisioned our workload cluster, but how do we access and use it?</p>
<p>Good question, when using ClusterAPI to spin up workload clusters, it needs to put the access credentials (i.e. the <code>kubeconfig</code> file) somewhere, so it puts them in a K8s <code>secret</code>, luckily they are very easy to retrieve and decode to your local machine.</p>
<div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash>kubectl get secret workload-cluster-01-kubeconfig -o<span class=o>=</span><span class=nv>jsonpath</span><span class=o>=</span><span class=s1>&#39;{.data.value}&#39;</span> <span class=p>|</span> <span class=o>{</span> base64 -d 2&gt;/dev/null <span class=o>||</span> base64 -D<span class=p>;</span> <span class=o>}</span> &gt;./out/workload-cluster-01/kubeconfig
</code></pre></div><p>Notice the <code>workload-cluster-01-kubeconfig</code> secret - this is what we want to connect to our workload cluster, it&rsquo;s very easy to extract and pull this to your local machine. The command pulls the <code>secret</code> value which is <code>base64</code> encoded in K8s - decodes it from <code>base64</code> to text and creates a new <code>kubeconfig</code> file in the workload cluster&rsquo;s directory on your laptop.</p>
<p>Let&rsquo;s apply the addons to our workload cluster (these are mainly just the networking overlay, Calico) - required to let pods talk to one-another - we will first change clusters by exporting <code>KUBECONFIG</code> once again:</p>
<div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=nb>export</span> <span class=nv>KUBECONFIG</span><span class=o>=</span>./out/workload-cluster-01/kubeconfig
kubectl apply -f ./out/workload-cluster-01/addons.yaml
</code></pre></div><p>And watch as the pods get spun up, when it&rsquo;s all working - everything should list as <code>Running</code>:.</p>
<div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash>kubectl get pods -n kube-system -w
</code></pre></div><h2 id=deploy-some-applications>Deploy some applications<a hidden class=anchor aria-hidden=true href=#deploy-some-applications>#</a></h2>
<p>Now that the workload cluster is set up, we can deploy some apps to it - because ClusterAPI also takes care of the CSI setup, we can even deploy ones that use persistent storage!</p>
<p>We&rsquo;re going to deploy use <code>helm</code> to set up an application called RocketChat on the cluster, which uses two persistent volumes, one for config and one for its MongoDB database.</p>
<h3 id=configure-helm>Configure helm<a hidden class=anchor aria-hidden=true href=#configure-helm>#</a></h3>
<p>Be aware this installation style for <code>helm</code> (granting the <code>tiller</code> pod <code>cluster-admin</code> privileges) is a <a href=https://github.com/helm/helm/blob/master/docs/securing_installation.md>big security no-no</a> and is just for ease of setup here. For more information on <a href=https://blog.ropnop.com/attacking-default-installs-of-helm-on-kubernetes/><em>why</em> this is bad, look here</a>, and please don&rsquo;t do this on a production cluster.</p>
<p>In this case, it is a throwaway cluster for me, so I will be using these permissions. First create the RBAC role and permissions for the <code>helm</code> service account in another new file called <code>helm-rbac.yaml</code>:</p>
<div class=highlight><pre tabindex=0 class=chroma><code class=language-yaml data-lang=yaml><span class=l>$ cat &lt;&lt;EOF &gt;helm-rbac.yaml</span><span class=w>
</span><span class=w></span><span class=nt>apiVersion</span><span class=p>:</span><span class=w> </span><span class=l>v1</span><span class=w>
</span><span class=w></span><span class=nt>kind</span><span class=p>:</span><span class=w> </span><span class=l>ServiceAccount</span><span class=w>
</span><span class=w></span><span class=nt>metadata</span><span class=p>:</span><span class=w>
</span><span class=w>  </span><span class=nt>name</span><span class=p>:</span><span class=w> </span><span class=l>tiller</span><span class=w>
</span><span class=w>  </span><span class=nt>namespace</span><span class=p>:</span><span class=w> </span><span class=l>kube-system</span><span class=w>
</span><span class=w></span><span class=nn>---</span><span class=w>
</span><span class=w></span><span class=nt>apiVersion</span><span class=p>:</span><span class=w> </span><span class=l>rbac.authorization.k8s.io/v1</span><span class=w>
</span><span class=w></span><span class=nt>kind</span><span class=p>:</span><span class=w> </span><span class=l>ClusterRoleBinding</span><span class=w>
</span><span class=w></span><span class=nt>metadata</span><span class=p>:</span><span class=w>
</span><span class=w>  </span><span class=nt>name</span><span class=p>:</span><span class=w> </span><span class=l>tiller</span><span class=w>
</span><span class=w></span><span class=nt>roleRef</span><span class=p>:</span><span class=w>
</span><span class=w>  </span><span class=nt>apiGroup</span><span class=p>:</span><span class=w> </span><span class=l>rbac.authorization.k8s.io</span><span class=w>
</span><span class=w>  </span><span class=nt>kind</span><span class=p>:</span><span class=w> </span><span class=l>ClusterRole</span><span class=w>
</span><span class=w>  </span><span class=nt>name</span><span class=p>:</span><span class=w> </span><span class=l>cluster-admin</span><span class=w>
</span><span class=w></span><span class=nt>subjects</span><span class=p>:</span><span class=w>
</span><span class=w>  </span>- <span class=nt>kind</span><span class=p>:</span><span class=w> </span><span class=l>ServiceAccount</span><span class=w>
</span><span class=w>    </span><span class=nt>name</span><span class=p>:</span><span class=w> </span><span class=l>tiller</span><span class=w>
</span><span class=w>    </span><span class=nt>namespace</span><span class=p>:</span><span class=w> </span><span class=l>kube-system</span><span class=w>
</span><span class=w></span><span class=l>EOF</span><span class=w>
</span></code></pre></div><p>Apply the role to the cluster:</p>
<div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash>kubectl apply -f helm-rbac.yaml
</code></pre></div><p>Let&rsquo;s install helm onto the cluster with the service account we provisioned:</p>
<div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash>helm init --service-account tiller
</code></pre></div><h3 id=configure-a-storageclass>Configure a StorageClass<a hidden class=anchor aria-hidden=true href=#configure-a-storageclass>#</a></h3>
<p>We&rsquo;re going to delete the default StorageClass that gets deployed and instead, create our own that uses the CSI plugin that is installed by default with CAPV.</p>
<div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash>kubectl delete sc --all
</code></pre></div><p>Create a new StorageClass <code>yaml</code> that uses the CSI provisioner (and by extension, CNS) - note the <code>provisioner</code> line:</p>
<div class=highlight><pre tabindex=0 class=chroma><code class=language-yaml data-lang=yaml><span class=l>$ cat &lt;&lt;EOF &gt;sc.yaml</span><span class=w>
</span><span class=w></span><span class=nt>kind</span><span class=p>:</span><span class=w> </span><span class=l>StorageClass</span><span class=w>
</span><span class=w></span><span class=nt>apiVersion</span><span class=p>:</span><span class=w> </span><span class=l>storage.k8s.io/v1</span><span class=w>
</span><span class=w></span><span class=nt>metadata</span><span class=p>:</span><span class=w>
</span><span class=w>  </span><span class=nt>name</span><span class=p>:</span><span class=w> </span><span class=l>standard</span><span class=w>
</span><span class=w>  </span><span class=nt>annotations</span><span class=p>:</span><span class=w>
</span><span class=w>    </span><span class=nt>storageclass.kubernetes.io/is-default-class</span><span class=p>:</span><span class=w> </span><span class=s2>&#34;true&#34;</span><span class=w>
</span><span class=w></span><span class=nt>provisioner</span><span class=p>:</span><span class=w> </span><span class=l>csi.vsphere.vmware.com</span><span class=w>
</span><span class=w></span><span class=nt>parameters</span><span class=p>:</span><span class=w>
</span><span class=w>    </span><span class=nt>storagePolicyName</span><span class=p>:</span><span class=w> </span><span class=s2>&#34;vSAN Default Storage Policy&#34;</span><span class=w>
</span><span class=w></span><span class=l>EOF</span><span class=w>
</span></code></pre></div><p>The above StorageClass uses the <code>vSAN Default Storage Policy</code> within vCenter, but you can change it to your own - the name of the SC in this case is <code>standard</code> and we&rsquo;ll use it deploying a demo app next.</p>
<div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash>kubectl apply -f sc.yaml
</code></pre></div><h3 id=provision-an-application>Provision an application<a hidden class=anchor aria-hidden=true href=#provision-an-application>#</a></h3>
<p>We&rsquo;re now in a place where we can provision an application, we&rsquo;re going to use <code>helm</code> to install RocketChat, as discussed above - RocketChat is basically an Open-Source Slack clone that you can run on-prem.</p>
<p>The below command tells <code>helm</code> to install RocketChat from the <code>stable/rocketchat</code> repository, give it a name, set the passwords for MongoDB and most critically - use the <code>standard StorageClass</code> that we just imported into the workload cluster to back the <code>PersistentVolumes</code> requested by RocketChat:</p>
<div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash>helm install stable/rocketchat --set persistence.StorageClass<span class=o>=</span>standard,mongodb.mongodbPassword<span class=o>=</span>password,mongodb.mongodbRootPassword<span class=o>=</span>password
</code></pre></div><p>Verify the volumes got provisioned (this will take a minute before it returns back the &ldquo;Bound&rdquo; status):</p>
<div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash>$ kubectl get pv,pvc
NAME                                                        CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS   CLAIM                                                      STORAGECLASS   REASON   AGE
persistentvolume/pvc-3c754fc9-f7bb-448f-8f2c-510fedc0cebc   8Gi        RWO            Delete           Bound    default/datadir-exacerbated-squirrel-mongodb-secondary-0   standard                19h
persistentvolume/pvc-47ad5e4d-4cdd-4c14-9148-5e9a2321bb8e   8Gi        RWO            Delete           Bound    default/datadir-exacerbated-squirrel-mongodb-primary-0     standard                19h

NAMESPACE   NAME                                                                     STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS   AGE
default     persistentvolumeclaim/datadir-exacerbated-squirrel-mongodb-primary-0     Bound    pvc-47ad5e4d-4cdd-4c14-9148-5e9a2321bb8e   8Gi        RWO            standard       19h
default     persistentvolumeclaim/datadir-exacerbated-squirrel-mongodb-secondary-0   Bound    pvc-3c754fc9-f7bb-448f-8f2c-510fedc0cebc   8Gi        RWO            standard       19h
</code></pre></div><p>And the pods for the application should be running:</p>
<div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash>$ kubectl get po
NAME                                             READY   STATUS             RESTARTS   AGE
exacerbated-squirrel-mongodb-arbiter-0           1/1     Running            <span class=m>0</span>          19h
exacerbated-squirrel-mongodb-primary-0           1/1     Running            <span class=m>0</span>          19h
exacerbated-squirrel-mongodb-secondary-0         1/1     Running            <span class=m>0</span>          19h
exacerbated-squirrel-rocketchat-958b577d-v8vzp   1/1     Running            <span class=m>0</span>          19h
</code></pre></div><p>At this point, we can access the application by port-forwarding to the <code>rocketchat-rocketchat-*</code> pod from the output above (change this to suit your pod name):</p>
<div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash>kubectl -port-forward exacerbated-squirrel-rocketchat-958b577d-v8vzp 8888:3000
</code></pre></div><p>Access the application on <code>localhost:8888</code> in your web browser:</p>
<div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash>open http://localhost:8888
</code></pre></div><p><img loading=lazy src=images/rocketchat-1.png alt="RocketChat UI">
</p>
<p>In the new CNS UI within vSphere 6.7 U3 - we can see the volumes that have been deployed from this K8s cluster via the CSI plugin:</p>
<p><img loading=lazy src=images/cns-vols.png alt="CNS Volumes">
</p>
<p>Why not follow <a href=images/rocketchat-1.png>@mylesagray on Twitter</a> for more like this!</p>
</div>
<footer class=post-footer>
<section class="section post-tags">
<div class=content>
<h2>Tagged with</h2>
</div>
<ul class=post-tags>
<li><a href=https://blah.cloud/tags/clusterapi/>clusterapi</a></li>
<li><a href=https://blah.cloud/tags/clusterapi-vsphere/>clusterapi vsphere</a></li>
<li><a href=https://blah.cloud/tags/cloud-native-storage/>cloud native storage</a></li>
<li><a href=https://blah.cloud/tags/kubernetes/>kubernetes</a></li>
<li><a href=https://blah.cloud/tags/vsphere/>vsphere</a></li>
</ul>
</section>
<nav class=paginav>
<a class=next href=https://blah.cloud/automation/using-velero-for-k8s-backup-and-restore-of-csi-volumes/>
<span class=title>Next Page »</span>
<br>
<span>Using Velero for K8s Backup and Restore of CSI Volumes</span>
</a>
</nav>
<section class="section related-links">
<div class="columns is-centered">
<div class="column max-800px">
<div class=content>
<h2>Related content</h2>
</div>
<div class="columns related-links-columns">
<div class="column is-one-third">
<div class=card>
<div class=card-image>
<figure class="image is-3by2">
<a href=/kubernetes/first-look-automated-k8s-lifecycle-with-clusterapi/><img src=/kubernetes/first-look-automated-k8s-lifecycle-with-clusterapi/images/featured-image.png alt></a>
</figure>
</div>
<div class=card-content>
<a class="title is-5" href=https://blah.cloud/kubernetes/first-look-automated-k8s-lifecycle-with-clusterapi/>First-look: Automated K8s lifecycle with ClusterAPI</a>
</div>
</div>
</div>
<div class="column is-one-third">
<div class=card>
<div class=card-image>
<figure class="image is-3by2">
<a href=/automation/using-velero-for-k8s-backup-and-restore-of-csi-volumes/><img src=/automation/using-velero-for-k8s-backup-and-restore-of-csi-volumes/images/minio-data.png alt></a>
</figure>
</div>
<div class=card-content>
<a class="title is-5" href=https://blah.cloud/automation/using-velero-for-k8s-backup-and-restore-of-csi-volumes/>Using Velero for K8s Backup and Restore of CSI Volumes</a>
</div>
</div>
</div>
<div class="column is-one-third">
<div class=card>
<div class=card-image>
<figure class="image is-3by2">
<a href=/kubernetes/using-the-vsphere-cloud-provider-for-k8s-to-dynamically-deploy-volumes/><img src=/kubernetes/using-the-vsphere-cloud-provider-for-k8s-to-dynamically-deploy-volumes/images/Screenshot-2019-01-27-13.42.27.png alt></a>
</figure>
</div>
<div class=card-content>
<a class="title is-5" href=https://blah.cloud/kubernetes/using-the-vsphere-cloud-provider-for-k8s-to-dynamically-deploy-volumes/>Using the vSphere Cloud Provider for K8s to dynamically deploy volumes</a>
</div>
</div>
</div>
</div>
</div>
</div>
</section>
<section class="section share-icons">
<div class=share-buttons>
<a target=_blank rel="noopener noreferrer" aria-label="share ClusterAPI for vSphere, now with CNS support on twitter" href="https://twitter.com/intent/tweet/?text=ClusterAPI%20for%20vSphere%2c%20now%20with%20CNS%20support&url=https%3a%2f%2fblah.cloud%2fkubernetes%2fclusterapi-for-vsphere-now-with-cns-support%2f&hashtags=clusterapi%2cclusterapivsphere%2ccloudnativestorage%2ckubernetes%2cvsphere"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM195.519 424.544c135.939.0 210.268-112.643 210.268-210.268.0-3.218.0-6.437-.153-9.502 14.406-10.421 26.973-23.448 36.935-38.314-13.18 5.824-27.433 9.809-42.452 11.648 15.326-9.196 26.973-23.602 32.49-40.92-14.252 8.429-30.038 14.56-46.896 17.931-13.487-14.406-32.644-23.295-53.946-23.295-40.767.0-73.87 33.104-73.87 73.87.0 5.824.613 11.494 1.992 16.858-61.456-3.065-115.862-32.49-152.337-77.241-6.284 10.881-9.962 23.601-9.962 37.088.0 25.594 13.027 48.276 32.95 61.456-12.107-.307-23.448-3.678-33.41-9.196v.92c0 35.862 25.441 65.594 59.311 72.49-6.13 1.686-12.72 2.606-19.464 2.606-4.751.0-9.348-.46-13.946-1.38 9.349 29.426 36.628 50.728 68.965 51.341-25.287 19.771-57.164 31.571-91.8 31.571-5.977.0-11.801-.306-17.625-1.073 32.337 21.15 71.264 33.41 112.95 33.41z"/></svg>
</a>
<a target=_blank rel="noopener noreferrer" aria-label="share ClusterAPI for vSphere, now with CNS support on reddit" href="https://reddit.com/submit?url=https%3a%2f%2fblah.cloud%2fkubernetes%2fclusterapi-for-vsphere-now-with-cns-support%2f&title=ClusterAPI%20for%20vSphere%2c%20now%20with%20CNS%20support"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg>
</a>
<a target=_blank rel="noopener noreferrer" aria-label="share ClusterAPI for vSphere, now with CNS support on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&url=https%3a%2f%2fblah.cloud%2fkubernetes%2fclusterapi-for-vsphere-now-with-cns-support%2f&title=ClusterAPI%20for%20vSphere%2c%20now%20with%20CNS%20support&summary=ClusterAPI%20for%20vSphere%2c%20now%20with%20CNS%20support&source=https%3a%2f%2fblah.cloud%2fkubernetes%2fclusterapi-for-vsphere-now-with-cns-support%2f"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg>
</a>
</div>
</section>
</footer><script src=https://utteranc.es/client.js repo=mylesagray/blog-comments issue-term=title theme=preferred-color-scheme crossorigin=anonymous async></script>
</article>
<aside class="hidden lg:block tableOfContentContainer" id=tableOfContentContainer>
<nav id=TableOfContents>
<ul>
<li><a href=#introduction>Introduction</a></li>
<li><a href=#prerequisites>Prerequisites</a>
<ul>
<li><a href=#tools>Tools</a></li>
</ul>
</li>
<li><a href=#environment-setup>Environment Setup</a>
<ul>
<li><a href=#pull-down-the-os-image>Pull down the OS image</a></li>
<li><a href=#set-up-vsphere-with-govc>Set up vSphere with govc</a></li>
<li><a href=#customise-and-import-the-template-vm>Customise and import the template VM</a></li>
</ul>
</li>
<li><a href=#using-clusterapi>Using ClusterAPI</a>
<ul>
<li><a href=#management-cluster>Management Cluster</a></li>
<li><a href=#workload-clusters>Workload Clusters</a></li>
</ul>
</li>
<li><a href=#connecting-to-the-workload-cluster>Connecting to the Workload Cluster</a></li>
<li><a href=#deploy-some-applications>Deploy some applications</a>
<ul>
<li><a href=#configure-helm>Configure helm</a></li>
<li><a href=#configure-a-storageclass>Configure a StorageClass</a></li>
<li><a href=#provision-an-application>Provision an application</a></li>
</ul>
</li>
</ul>
</nav>
</aside>
</div>
</main>
</div>
<footer class=footer>
<span>&copy; 2021 <a href=https://blah.cloud/>Blah, Cloud</a></span>
<span>
Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://git.io/hugopapermod rel=noopener target=_blank>PaperMod</a>
</span>
</footer>
<a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a>
<script>let menu=document.getElementById('menu');menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)},document.querySelectorAll('a[href^="#"]').forEach(a=>{a.addEventListener("click",function(b){b.preventDefault();var a=this.getAttribute("href").substr(1);window.matchMedia('(prefers-reduced-motion: reduce)').matches?document.querySelector(`[id='${decodeURIComponent(a)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(a)}']`).scrollIntoView({behavior:"smooth"}),a==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${a}`)})})</script>
<script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script>
<script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove('dark'),localStorage.setItem("pref-theme",'light')):(document.body.classList.add('dark'),localStorage.setItem("pref-theme",'dark'))})</script>
<script>document.querySelectorAll('pre > code').forEach(b=>{const c=b.parentNode.parentNode,a=document.createElement('button');a.classList.add('copy-code'),a.innerText='copy';function d(){a.innerText='copied!',setTimeout(()=>{a.innerText='copy'},2e3)}a.addEventListener('click',e=>{if('clipboard'in navigator){navigator.clipboard.writeText(b.textContent),d();return}const a=document.createRange();a.selectNodeContents(b);const c=window.getSelection();c.removeAllRanges(),c.addRange(a);try{document.execCommand('copy'),d()}catch(a){}c.removeRange(a)}),c.classList.contains("highlight")?c.appendChild(a):c.parentNode.firstChild==c||(b.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?b.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(a):b.parentNode.appendChild(a))})</script>
</body>
</html>