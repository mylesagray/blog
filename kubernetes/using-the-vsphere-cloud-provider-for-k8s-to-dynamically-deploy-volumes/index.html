<!doctype html><html lang=en dir=auto>
<head><meta charset=utf-8>
<meta http-equiv=x-ua-compatible content="IE=edge">
<meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no">
<meta name=robots content="index, follow">
<title>Using the vSphere Cloud Provider for K8s to dynamically deploy volumes | Blah, Cloud</title>
<meta name=keywords content="containers,esxi,helm,kubernetes,vmware,vsphere,persistent storage">
<meta name=description content="How to provision Kubernetes Persistent Volumes dynamically on vSphere">
<meta name=author content="Myles Gray">
<link rel=canonical href=https://beta.blah.cloud/kubernetes/using-the-vsphere-cloud-provider-for-k8s-to-dynamically-deploy-volumes/>
<link crossorigin=anonymous href=/assets/css/stylesheet.min.e38a41c5e7d4d3ea02d394ee536b3395f1cb27bd5a9162de1e375c7d2d1bc8f0.css integrity="sha256-44pBxefU0+oC05TuU2szlfHLJ71akWLeHjdcfS0byPA=" rel="preload stylesheet" as=style>
<link rel=preload href=/images/logo-title.png as=image>
<script defer crossorigin=anonymous src=/assets/js/scroll-toc.min.a4bed116d37981ac4d9e6d35b2e80495aaddc43e6e6191cfd20f1d510a738b65.js integrity="sha256-pL7RFtN5gaxNnm01sugElardxD5uYZHP0g8dUQpzi2U="></script>
<link rel=icon href=https://beta.blah.cloud/images/favicon.ico>
<link rel=icon type=image/png sizes=16x16 href=https://beta.blah.cloud/images/favicon-16x16.png>
<link rel=icon type=image/png sizes=32x32 href=https://beta.blah.cloud/images/favicon-32x32.png>
<link rel=apple-touch-icon href=https://beta.blah.cloud/images/apple-touch-icon.png>
<link rel=mask-icon href=https://beta.blah.cloud/images/logo.png>
<meta name=theme-color content="#2e2e33">
<meta name=msapplication-TileColor content="#2e2e33">
<meta name=generator content="Hugo 0.88.1">
<link rel=amphtml type=text/html href=https://beta.blah.cloud/amp/kubernetes/using-the-vsphere-cloud-provider-for-k8s-to-dynamically-deploy-volumes/>
<meta property="og:title" content="Using the vSphere Cloud Provider for K8s to dynamically deploy volumes">
<meta property="og:description" content="How to provision Kubernetes Persistent Volumes dynamically on vSphere">
<meta property="og:type" content="article">
<meta property="og:url" content="https://beta.blah.cloud/kubernetes/using-the-vsphere-cloud-provider-for-k8s-to-dynamically-deploy-volumes/">
<meta property="og:image" content="https://beta.blah.cloud/images/Screenshot-2019-01-27-13.42.27.png"><meta property="article:section" content="posts">
<meta property="article:published_time" content="2019-02-08T22:38:35+00:00">
<meta property="article:modified_time" content="2021-10-25T15:11:00+00:00"><meta property="og:site_name" content="Blah, Cloud">
<meta property="og:see_also" content="https://beta.blah.cloud/kubernetes/clusterapi-for-vsphere-now-with-cns-support/"><meta property="og:see_also" content="https://beta.blah.cloud/automation/using-velero-for-k8s-backup-and-restore-of-csi-volumes/"><meta property="og:see_also" content="https://beta.blah.cloud/kubernetes/first-look-automated-k8s-lifecycle-with-clusterapi/"><meta property="og:see_also" content="https://beta.blah.cloud/infrastructure/using-cloud-init-for-vm-templating-on-vsphere/"><meta property="og:see_also" content="https://beta.blah.cloud/kubernetes/setting-up-k8s-and-the-vsphere-cloud-provider-using-kubeadm/">
<meta name=twitter:card content="summary_large_image">
<meta name=twitter:image content="https://beta.blah.cloud/images/Screenshot-2019-01-27-13.42.27.png">
<meta name=twitter:title content="Using the vSphere Cloud Provider for K8s to dynamically deploy volumes">
<meta name=twitter:description content="How to provision Kubernetes Persistent Volumes dynamically on vSphere">
<script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":2,"name":"Using the vSphere Cloud Provider for K8s to dynamically deploy volumes","item":"https://beta.blah.cloud/kubernetes/using-the-vsphere-cloud-provider-for-k8s-to-dynamically-deploy-volumes/"}]}</script>
<script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Using the vSphere Cloud Provider for K8s to dynamically deploy volumes","name":"Using the vSphere Cloud Provider for K8s to dynamically deploy volumes","description":"How to provision Kubernetes Persistent Volumes dynamically on vSphere","keywords":["containers","esxi","helm","kubernetes","vmware","vsphere","persistent storage"],"articleBody":"Using the VCP As of the last part in the series we have a fully up and running k8s cluster with the vSphere Cloud Provider installed! Let’s make sure it works and is provisioning storage for us by deploying a StorageClass and a test app.\nPrerequisites Tools I am using macOS, so will be using the brew package manager to install and manage my tools, if you are using Linux or Windows, use the appropriate install guide for each tool, according to your OS.\nFor each tool I will list the brew install command and the link to the install instructions for other OSes.\n brew  https://brew.sh   helm - brew install kubernetes-helm  https://helm.sh   kubectl - brew install kubernetes-cli  https://kubernetes.io/docs/tasks/tools/install-kubectl/    vCenter In vCenter you should already have a Storage Policy created for whatever datastore(s) you are using, this can be with richer policy primitives when using vSAN (things like RAID method, numbers of replicas, etc) or if using standard NFS/VMFS datastores, tag-based placement works as well.\nI am using the “vSAN Default Storage Policy” as below, which does what it says on the tin, is a default policy that infers RAID-1 mirroring and single failure tolerance.\nKubernetes Put the below into a new yaml file (i’m calling mine vsan-default-storage-policy.yaml) - it will create a new StorageClass in k8s called vsan-default that maps to the vSAN SPBM policy vSAN Default Storage Policy.\nNote the annotation storageclass.kubernetes.io/is-default-class: \"true\" means that anything deployed that requires a PersistentVolume that doesn’t specify a StorageClass will use this one as its fallback. In other words, it’s the catch-all.\nkind:StorageClassapiVersion:storage.k8s.io/v1metadata:name:vsan-defaultannotations:storageclass.kubernetes.io/is-default-class:\"true\"provisioner:kubernetes.io/vsphere-volumeparameters:storagePolicyName:\"vSAN Default Storage Policy\"datastore:vsanDatastoreApply it to the cluster:\nkubectl create -f vsan-default-storage-policy.yaml Let’s deploy a stateful app to test it with, for this to keep things simple, we will use helm (think of it as an application manager, for K8s). Be aware this installation style for helm (granting the tiller pod cluster-admin privileges) is a big security no-no and is just for ease of setup here. For more information on why this is bad, look here, and please don’t do this on a production cluster.\nIn this case, it is a throwaway cluster for me, so I will be using these permissions. First create the RBAC role and permissions for the helm service account in another new file called helm-rbac.yaml:\napiVersion:v1kind:ServiceAccountmetadata:name:tillernamespace:kube-system---apiVersion:rbac.authorization.k8s.io/v1kind:ClusterRoleBindingmetadata:name:tillerroleRef:apiGroup:rbac.authorization.k8s.iokind:ClusterRolename:cluster-adminsubjects:- kind:ServiceAccountname:tillernamespace:kube-systemApply the role to the cluster:\n$ kubectl create -f helm-rbac.yaml serviceaccount/tiller created clusterrolebinding.rbac.authorization.k8s.io/tiller created Let’s install helm onto the cluster with the service account we provisioned:\n$ helm init --service-account tiller $HELM_HOME has been configured at /Users/mylesgray/.helm. Tiller (the Helm server-side component) has been installed into your Kubernetes Cluster. Please note: by default, Tiller is deployed with an insecure 'allow unauthenticated users' policy. To prevent this, run `helm init` with the --tiller-tls-verify flag. For more information on securing your installation see: https://docs.helm.sh/using_helm/#securing-your-helm-installation Happy Helming! And update helm from the chart repositories\n$ helm repo update Hang tight while we grab the latest from your chart repositories... ...Skip local chart repository ...Successfully got an update from the \"incubator\" chart repository ...Successfully got an update from the \"istio.io\" chart repository ...Successfully got an update from the \"gitlab\" chart repository ...Successfully got an update from the \"openfaas\" chart repository ...Successfully got an update from the \"stable\" chart repository Update Complete. ⎈ Happy Helming!⎈ At last, let’s deploy the chart for mongodb to test our installation (which will use our default StorageClass we just created)\n$ helm install --name test-mongodb stable/mongodb NAME: test-mongodb LAST DEPLOYED: Sat Jan 26 20:54:53 2019 NAMESPACE: default STATUS: DEPLOYED RESOURCES: == v1/Secret NAME TYPE DATA AGE test-mongodb Opaque 1 2s == v1/PersistentVolumeClaim NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE test-mongodb Pending vsan-default 2s == v1/Service NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE test-mongodb ClusterIP 10.98.144.241  27017/TCP 2s == v1beta1/Deployment NAME DESIRED CURRENT UP-TO-DATE AVAILABLE AGE test-mongodb 1 1 1 0 2s == v1/Pod(related) NAME READY STATUS RESTARTS AGE test-mongodb-646b949fd4-xjxdb 0/1 Pending 0 2s NOTES: ** Please be patient while the chart is being deployed ** MongoDB can be accessed via port 27017 on the following DNS name from within your cluster: test-mongodb.default.svc.cluster.local To get the root password run: export MONGODB_ROOT_PASSWORD=$(kubectl get secret --namespace default test-mongodb -o jsonpath=\"{.data.mongodb-root-password}\" | base64 --decode) To connect to your database run the following command: kubectl run --namespace default test-mongodb-client --rm --tty -i --restart='Never' --image bitnami/mongodb --command -- mongo admin --host test-mongodb --authenticationDatabase admin -u root -p $MONGODB_ROOT_PASSWORD To connect to your database from outside the cluster execute the following commands: kubectl port-forward --namespace default svc/test-mongodb 27017:27017 \u0026 mongo --host 127.0.0.1 --authenticationDatabase admin -p $MONGODB_ROOT_PASSWORD Check to make sure the PersistentVolume and PersistentVolumeClaim deployed successfully, they should show a status of Bound if they have. (You may need to run this a few times while the volume provisions and gets mounted)\n$ kubectl get pv,pvc NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGE persistentvolume/pvc-64941983-21b5-11e9-b851-005056b9750f 8Gi RWO Delete Bound default/test-mongodb vsan-default 63m NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE persistentvolumeclaim/test-mongodb Bound pvc-64941983-21b5-11e9-b851-005056b9750f 8Gi RWO vsan-default 63m Monitor the app’s deployment and wait for all items to show as Running\n$ kubectl get all NAME READY STATUS RESTARTS AGE pod/test-mongodb-646b949fd4-cz65g 1/1 Running 0 62m NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/kubernetes ClusterIP 10.96.0.1  443/TCP 3h20m service/test-mongodb ClusterIP 10.97.102.136  27017/TCP 62m NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/test-mongodb 1/1 1 1 62m NAME DESIRED CURRENT READY AGE replicaset.apps/test-mongodb-646b949fd4 1 1 1 62m Verify the app works by testing access from the master node by spinning up a client container\n# Export the password set during deployment export MONGODB_ROOT_PASSWORD=$(kubectl get secret --namespace default test-mongodb -o jsonpath=\"{.data.mongodb-root-password}\" | base64 --decode) # Connect a client container to your server container kubectl run --namespace default test-mongodb-client --rm --tty -i --restart='Never' --image bitnami/mongodb --command -- mongo admin --host test-mongodb --authenticationDatabase admin -u root -p $MONGODB_ROOT_PASSWORD # Show databases  show dbs admin 0.000GB config 0.000GB local 0.000GB  exit At this stage it is worth installing the kubernetes dashboard as well, for some visibility.\nkubectl apply -f https://raw.githubusercontent.com/kubernetes/dashboard/v1.10.1/src/deploy/recommended/kubernetes-dashboard.yaml Create a user (ServiceAccount) for accessing the UI with a new yaml file called ui-user.yaml (as you can see, I called mine myles)\napiVersion:v1kind:ServiceAccountmetadata:name:mylesnamespace:kube-systemCreate a cluster-role for the user (myles) such that is has cluster-admin privileges - my file is called clusterrolebinding.yaml\napiVersion:rbac.authorization.k8s.io/v1kind:ClusterRoleBindingmetadata:name:mylesroleRef:apiGroup:rbac.authorization.k8s.iokind:ClusterRolename:cluster-adminsubjects:- kind:ServiceAccountname:mylesnamespace:kube-systemImport both configs into K8s\nkubectl create -f ui-user.yaml kubectl create -f clusterrolebinding.yaml Notice, we didn’t create a password - they are generated automatically by K8S, so let’s get the access token for the user we just created (change the grep section from myles in the command to reflect the username you used in ui-user.yaml above) - you will need to copy and paste the token: output from the command.\n$ kubectl -n kube-system describe secret $(kubectl -n kube-system get secret | grep myles | awk '{print $1}') Name: myles-token-2mkr6 Namespace: kube-system Labels:  Annotations: kubernetes.io/service-account.name: myles kubernetes.io/service-account.uid: 95920d01-21c9-11e9-b851-005056b9750f Type: kubernetes.io/service-account-token Data ==== token: eyJhbGciOiJSUzI1NiIsImtpZCI6IiJ9.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJrdWJlLXN5c3RlbSIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VjcmV0Lm5hbWUiOiJteWxlcy10b2tlbi0ybWtyNiIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VydmljZS1hY2NvdW50Lm5hbWUiOiJteWxlcyIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VydmljZS1hY2NvdW50LnVpZCI6Ijk1OTIwZDAxLTIxYzktMTFlOS1iODUxLTAwNTA1NmI5NzUwZiIsInN1YiI6InN5c3RlbTpzZXJ2aWNlYWNjb3VudDprdWJlLXN5c3RlbTpteWxlcyJ9.YzVRG6Dt_p4_r7Uc7tCAEXjRG8xaB5HqeSO9DdcaXQWf6mqGhH2ahiXI3XdkqOm2725NHEJUsErD8GrJpGYnL_od15Zvxhn1D4VZr3Q3ds-nJ0IK2KS_ArXj3bypO6sjAEBb7bXviuWxge0bLlkurnuLYQSa9lrijHe95AGJnNrDi66Dr1eQoE4deJrjX7Bxm6ef2tikl6lCRA69Q57glQFBQm2aIvOUvR3y5b16vIVMQ6dJcnSE1EjB-G0n0lLRUzPij2nNU7IvmUBEzIbY2jFBxYiY8PEi0sMB2MZSitnW7DbMlZ5Yb8anHsX2XJbixl-VoDkMJuyujzwIc6vs8Q ca.crt: 1025 bytes namespace: 11 bytes Create a proxy session to K8s to access the UI - note: by default, the dashboard is not accessible external to the cluster more detail on how to do that here\nkubectl proxy \u0026 Then access the dashboard (details on how to authenticate are available here):\nopen http://localhost:8001/api/v1/namespaces/kube-system/services/https:kubernetes-dashboard:/proxy/ Change the access mode to token and paste in the output we copied above, profit\nAt this point you can browse around the K8s dashboard, view the ReplicaSet that helm created for mongodb, view the PersistentVolume created for us automatically from the StorageClass we defined and a lot more.\nFeel free to have a look around, dive back into helm and deploy some more apps!\nWhy not follow @mylesagray on Twitter for more like this!\n","wordCount":"1270","inLanguage":"en","image":"https://beta.blah.cloud/images/Screenshot-2019-01-27-13.42.27.png","datePublished":"2019-02-08T22:38:35Z","dateModified":"2021-10-25T15:11:00Z","author":{"@type":"Person","name":"Myles Gray"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://beta.blah.cloud/kubernetes/using-the-vsphere-cloud-provider-for-k8s-to-dynamically-deploy-volumes/"},"publisher":{"@type":"Organization","name":"Blah, Cloud","logo":{"@type":"ImageObject","url":"https://beta.blah.cloud/images/favicon.ico"}}}</script>
<script defer data-domain=blah.cloud src=https://plausible.io/js/plausible.js></script>
<script>window.plausible=window.plausible||function(){(window.plausible.q=window.plausible.q||[]).push(arguments)}</script>
</head>
<body id=top>
<script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add('dark'):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove('dark'):window.matchMedia('(prefers-color-scheme: dark)').matches&&document.body.classList.add('dark')</script>
<script>var themeColor=getComputedStyle(document.body).getPropertyValue('--primary');document.querySelector('meta[name="theme-color"]').setAttribute('content',themeColor)</script>
<noscript>
<style type=text/css>#theme-toggle,.top-link{display:none}</style>
<style>@media(prefers-color-scheme:dark){:root{--theme:#1d1e20;--entry:#2e2e33;--primary:rgba(240, 202, 102, 1);--secondary:rgba(216, 214, 197, 1);--tertiary:rgba(128, 130 ,133 , 1);--content:rgba(206, 205, 188, 1);--hljs-bg:#282a36;--code-bg:#282a36;--border:rgba(240, 202, 102, 1)}.list{background:var(--theme)}}</style>
</noscript>
<header class=header>
<nav class=nav>
<div class=logo>
<a href=https://beta.blah.cloud/ accesskey=h title="Blah, Cloud. (Alt + H)">
<img src=/images/logo-title.png alt=logo aria-label=logo height=40></a>
</div>
<span class=logo-switches>
<button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg>
</button>
</span>
<ul id=menu>
<li>
<a href=https://beta.blah.cloud/blog/ title=blog>
<span>blog</span>
</a>
</li>
<li>
<a href=https://beta.blah.cloud/works/ title=works>
<span>works</span>
</a>
</li>
<li>
<a href=https://beta.blah.cloud/about/ title=about>
<span>about</span>
</a>
</li>
<li>
<a href=https://beta.blah.cloud/search/ title=" (Alt + /)" accesskey=/>
<span><svg style="height:1em;margin-top:1.5em" aria-hidden="true" focusable="false" data-prefix="fas" data-icon="search" class="fill-current w-5" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path d="M505 442.7 405.3 343c-4.5-4.5-10.6-7-17-7H372c27.6-35.3 44-79.7 44-128C416 93.1 322.9.0 208 0S0 93.1.0 208s93.1 208 208 208c48.3.0 92.7-16.4 128-44v16.3c0 6.4 2.5 12.5 7 17l99.7 99.7c9.4 9.4 24.6 9.4 33.9.0l28.3-28.3c9.4-9.4 9.4-24.6.1-34zM208 336c-70.7.0-128-57.2-128-128 0-70.7 57.2-128 128-128 70.7.0 128 57.2 128 128 0 70.7-57.2 128-128 128z"/></svg></span>
</a>
</li>
</ul>
</nav>
</header>
<div class=container>
<main class=main>
<div class=wrapper>
<article class=post-single>
<header class=post-header>
<h1 class=post-title>
Using the vSphere Cloud Provider for K8s to dynamically deploy volumes
</h1>
<div class=post-description>
How to provision Kubernetes Persistent Volumes dynamically on vSphere
</div>
<div class=post-meta>February 8, 2019&nbsp;·&nbsp;Myles Gray&nbsp;|&nbsp;<a href=https://github.com/mylesagray/blog/blob/master/content/posts/2019-02-08-using-the-vsphere-cloud-provider-for-k8s-to-dynamically-deploy-volumes/index.md rel="noopener noreferrer" target=_blank>Suggest Changes</a>
</div>
</header>
<figure class=entry-cover>
<img loading=lazy srcset="https://beta.blah.cloud/kubernetes/using-the-vsphere-cloud-provider-for-k8s-to-dynamically-deploy-volumes/images/Screenshot-2019-01-27-13.42.27_hud8033d7fc39907f7f1244062de1840dd_99916_360x0_resize_box_3.png 360w ,https://beta.blah.cloud/kubernetes/using-the-vsphere-cloud-provider-for-k8s-to-dynamically-deploy-volumes/images/Screenshot-2019-01-27-13.42.27_hud8033d7fc39907f7f1244062de1840dd_99916_480x0_resize_box_3.png 480w ,https://beta.blah.cloud/kubernetes/using-the-vsphere-cloud-provider-for-k8s-to-dynamically-deploy-volumes/images/Screenshot-2019-01-27-13.42.27_hud8033d7fc39907f7f1244062de1840dd_99916_720x0_resize_box_3.png 720w ,https://beta.blah.cloud/kubernetes/using-the-vsphere-cloud-provider-for-k8s-to-dynamically-deploy-volumes/images/Screenshot-2019-01-27-13.42.27_hud8033d7fc39907f7f1244062de1840dd_99916_1080x0_resize_box_3.png 1080w ,https://beta.blah.cloud/kubernetes/using-the-vsphere-cloud-provider-for-k8s-to-dynamically-deploy-volumes/images/Screenshot-2019-01-27-13.42.27.png 1280w" sizes="(min-width: 768px) 720px, 100vw" src=https://beta.blah.cloud/kubernetes/using-the-vsphere-cloud-provider-for-k8s-to-dynamically-deploy-volumes/images/Screenshot-2019-01-27-13.42.27.png alt="K8s Dashboard" width=1280 height=754>
</figure><div class=toc>
<details>
<summary accesskey=c title="(Alt + C)">
<span class=details>Table of Contents</span>
</summary>
<div class=inner><ul>
<li>
<a href=#using-the-vcp aria-label="Using the VCP">Using the VCP</a></li>
<li>
<a href=#prerequisites aria-label=Prerequisites>Prerequisites</a><ul>
<li>
<a href=#tools aria-label=Tools>Tools</a></li></ul>
</li>
<li>
<a href=#vcenter aria-label=vCenter>vCenter</a></li>
<li>
<a href=#kubernetes aria-label=Kubernetes>Kubernetes</a>
</li>
</ul>
</div>
</details>
</div>
<div class=post-content><h2 id=using-the-vcp>Using the VCP<a hidden class=anchor aria-hidden=true href=#using-the-vcp>#</a></h2>
<p>As of the <a href=/kubernetes/setting-up-k8s-and-the-vsphere-cloud-provider-using-kubeadm/>last part in the series</a> we have a fully up and running k8s cluster with the vSphere Cloud Provider installed! Let&rsquo;s make sure it works and is provisioning storage for us by deploying a <code>StorageClass</code> and a test app.</p>
<h2 id=prerequisites>Prerequisites<a hidden class=anchor aria-hidden=true href=#prerequisites>#</a></h2>
<h3 id=tools>Tools<a hidden class=anchor aria-hidden=true href=#tools>#</a></h3>
<p>I am using macOS, so will be using the <code>brew</code> package manager to install and manage my tools, if you are using Linux or Windows, use the appropriate install guide for each tool, according to your OS.</p>
<p>For each tool I will list the <code>brew</code> install command and the link to the install instructions for other OSes.</p>
<ul>
<li>brew
<ul>
<li><a href=https://brew.sh>https://brew.sh</a></li>
</ul>
</li>
<li>helm - <code>brew install kubernetes-helm</code>
<ul>
<li><a href=https://helm.sh>https://helm.sh</a></li>
</ul>
</li>
<li>kubectl - <code>brew install kubernetes-cli</code>
<ul>
<li><a href=https://kubernetes.io/docs/tasks/tools/install-kubectl/>https://kubernetes.io/docs/tasks/tools/install-kubectl/</a></li>
</ul>
</li>
</ul>
<h2 id=vcenter>vCenter<a hidden class=anchor aria-hidden=true href=#vcenter>#</a></h2>
<p>In vCenter you should already have a Storage Policy created for whatever datastore(s) you are using, this can be with <a href=https://blogs.vmware.com/virtualblocks/2018/11/30/managing-your-data-on-vsan-with-spbm/>richer policy primitives</a> when using vSAN (things like RAID method, numbers of replicas, etc) or if using standard NFS/VMFS datastores, <a href=https://blogs.vmware.com/virtualblocks/2018/07/26/using-tag-based-spbm-policies-to-manage-your-storage/>tag-based placement</a> works as well.</p>
<p>I am using the &ldquo;vSAN Default Storage Policy&rdquo; as below, which does what it says on the tin, is a default policy that infers RAID-1 mirroring and single failure tolerance.</p>
<p><img loading=lazy src=https://dl.dropboxusercontent.com/s/jgacnf6tkqlj2rn/vSAN-Default-SPBM.png alt="vSAN SPBM Policy">
</p>
<h2 id=kubernetes>Kubernetes<a hidden class=anchor aria-hidden=true href=#kubernetes>#</a></h2>
<p>Put the below into a new yaml file (i&rsquo;m calling mine <code>vsan-default-storage-policy.yaml</code>) - it will create a new <code>StorageClass</code> in k8s called <code>vsan-default</code> that maps to the vSAN SPBM policy <code>vSAN Default Storage Policy</code>.</p>
<p>Note the annotation <code>storageclass.kubernetes.io/is-default-class: "true"</code> means that anything deployed that requires a <code>PersistentVolume</code> that doesn&rsquo;t specify a <code>StorageClass</code> will use this one as its fallback. In other words, it&rsquo;s the catch-all.</p>
<div class=highlight><pre tabindex=0 class=chroma><code class=language-yaml data-lang=yaml><span class=nt>kind</span><span class=p>:</span><span class=w> </span><span class=l>StorageClass</span><span class=w>
</span><span class=w></span><span class=nt>apiVersion</span><span class=p>:</span><span class=w> </span><span class=l>storage.k8s.io/v1</span><span class=w>
</span><span class=w></span><span class=nt>metadata</span><span class=p>:</span><span class=w>
</span><span class=w>  </span><span class=nt>name</span><span class=p>:</span><span class=w> </span><span class=l>vsan-default</span><span class=w>
</span><span class=w>  </span><span class=nt>annotations</span><span class=p>:</span><span class=w>
</span><span class=w>    </span><span class=nt>storageclass.kubernetes.io/is-default-class</span><span class=p>:</span><span class=w> </span><span class=s2>&#34;true&#34;</span><span class=w>
</span><span class=w></span><span class=nt>provisioner</span><span class=p>:</span><span class=w> </span><span class=l>kubernetes.io/vsphere-volume</span><span class=w>
</span><span class=w></span><span class=nt>parameters</span><span class=p>:</span><span class=w>
</span><span class=w>    </span><span class=nt>storagePolicyName</span><span class=p>:</span><span class=w> </span><span class=s2>&#34;vSAN Default Storage Policy&#34;</span><span class=w>
</span><span class=w>    </span><span class=nt>datastore</span><span class=p>:</span><span class=w> </span><span class=l>vsanDatastore</span><span class=w>
</span></code></pre></div><p>Apply it to the cluster:</p>
<div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash>kubectl create -f vsan-default-storage-policy.yaml
</code></pre></div><p>Let&rsquo;s deploy a stateful app to test it with, for this to keep things simple, we will use <code>helm</code> (think of it as an application manager, for K8s). Be aware this installation style for <code>helm</code> (granting the <code>tiller</code> pod <code>cluster-admin</code> privileges) is a <a href=https://github.com/helm/helm/blob/master/docs/securing_installation.md>big security no-no</a> and is just for ease of setup here. For more information on <a href=https://blog.ropnop.com/attacking-default-installs-of-helm-on-kubernetes/><em>why</em> this is bad, look here</a>, and please don&rsquo;t do this on a production cluster.</p>
<p>In this case, it is a throwaway cluster for me, so I will be using these permissions. First create the RBAC role and permissions for the <code>helm</code> service account in another new file called <code>helm-rbac.yaml</code>:</p>
<div class=highlight><pre tabindex=0 class=chroma><code class=language-yaml data-lang=yaml><span class=nt>apiVersion</span><span class=p>:</span><span class=w> </span><span class=l>v1</span><span class=w>
</span><span class=w></span><span class=nt>kind</span><span class=p>:</span><span class=w> </span><span class=l>ServiceAccount</span><span class=w>
</span><span class=w></span><span class=nt>metadata</span><span class=p>:</span><span class=w>
</span><span class=w>  </span><span class=nt>name</span><span class=p>:</span><span class=w> </span><span class=l>tiller</span><span class=w>
</span><span class=w>  </span><span class=nt>namespace</span><span class=p>:</span><span class=w> </span><span class=l>kube-system</span><span class=w>
</span><span class=w></span><span class=nn>---</span><span class=w>
</span><span class=w></span><span class=nt>apiVersion</span><span class=p>:</span><span class=w> </span><span class=l>rbac.authorization.k8s.io/v1</span><span class=w>
</span><span class=w></span><span class=nt>kind</span><span class=p>:</span><span class=w> </span><span class=l>ClusterRoleBinding</span><span class=w>
</span><span class=w></span><span class=nt>metadata</span><span class=p>:</span><span class=w>
</span><span class=w>  </span><span class=nt>name</span><span class=p>:</span><span class=w> </span><span class=l>tiller</span><span class=w>
</span><span class=w></span><span class=nt>roleRef</span><span class=p>:</span><span class=w>
</span><span class=w>  </span><span class=nt>apiGroup</span><span class=p>:</span><span class=w> </span><span class=l>rbac.authorization.k8s.io</span><span class=w>
</span><span class=w>  </span><span class=nt>kind</span><span class=p>:</span><span class=w> </span><span class=l>ClusterRole</span><span class=w>
</span><span class=w>  </span><span class=nt>name</span><span class=p>:</span><span class=w> </span><span class=l>cluster-admin</span><span class=w>
</span><span class=w></span><span class=nt>subjects</span><span class=p>:</span><span class=w>
</span><span class=w>  </span>- <span class=nt>kind</span><span class=p>:</span><span class=w> </span><span class=l>ServiceAccount</span><span class=w>
</span><span class=w>    </span><span class=nt>name</span><span class=p>:</span><span class=w> </span><span class=l>tiller</span><span class=w>
</span><span class=w>    </span><span class=nt>namespace</span><span class=p>:</span><span class=w> </span><span class=l>kube-system</span><span class=w>
</span></code></pre></div><p>Apply the role to the cluster:</p>
<div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash>$ kubectl create -f helm-rbac.yaml
serviceaccount/tiller created
clusterrolebinding.rbac.authorization.k8s.io/tiller created
</code></pre></div><p>Let&rsquo;s install helm onto the cluster with the service account we provisioned:</p>
<div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash>$ helm init --service-account tiller
<span class=nv>$HELM_HOME</span> has been configured at /Users/mylesgray/.helm.

Tiller <span class=o>(</span>the Helm server-side component<span class=o>)</span> has been installed into your Kubernetes Cluster.

Please note: by default, Tiller is deployed with an insecure <span class=s1>&#39;allow unauthenticated users&#39;</span> policy.
To prevent this, run <span class=sb>`</span>helm init<span class=sb>`</span> with the --tiller-tls-verify flag.
For more information on securing your installation see: https://docs.helm.sh/using_helm/#securing-your-helm-installation
Happy Helming!
</code></pre></div><p>And update helm from the chart repositories</p>
<div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash>$ helm repo update
Hang tight <span class=k>while</span> we grab the latest from your chart repositories...
...Skip <span class=nb>local</span> chart repository
...Successfully got an update from the <span class=s2>&#34;incubator&#34;</span> chart repository
...Successfully got an update from the <span class=s2>&#34;istio.io&#34;</span> chart repository
...Successfully got an update from the <span class=s2>&#34;gitlab&#34;</span> chart repository
...Successfully got an update from the <span class=s2>&#34;openfaas&#34;</span> chart repository
...Successfully got an update from the <span class=s2>&#34;stable&#34;</span> chart repository
Update Complete. ⎈ Happy Helming!⎈ 
</code></pre></div><p>At last, let&rsquo;s deploy the chart for mongodb to test our installation (which will use our default <code>StorageClass</code> we just created)</p>
<div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash>$ helm install --name test-mongodb stable/mongodb

NAME:   test-mongodb
LAST DEPLOYED: Sat Jan <span class=m>26</span> 20:54:53 <span class=m>2019</span>
NAMESPACE: default
STATUS: DEPLOYED

RESOURCES:
<span class=o>==</span>&gt; v1/Secret
NAME          TYPE    DATA  AGE
test-mongodb  Opaque  <span class=m>1</span>     <span class=nv>2s</span>

<span class=o>==</span>&gt; v1/PersistentVolumeClaim
NAME          STATUS   VOLUME        CAPACITY  ACCESS MODES  STORAGECLASS  AGE
test-mongodb  Pending  vsan-default  <span class=nv>2s</span>

<span class=o>==</span>&gt; v1/Service
NAME          TYPE       CLUSTER-IP     EXTERNAL-IP  PORT<span class=o>(</span>S<span class=o>)</span>    AGE
test-mongodb  ClusterIP  10.98.144.241  &lt;none&gt;       27017/TCP  <span class=nv>2s</span>

<span class=o>==</span>&gt; v1beta1/Deployment
NAME          DESIRED  CURRENT  UP-TO-DATE  AVAILABLE  AGE
test-mongodb  <span class=m>1</span>        <span class=m>1</span>        <span class=m>1</span>           <span class=m>0</span>          <span class=nv>2s</span>

<span class=o>==</span>&gt; v1/Pod<span class=o>(</span>related<span class=o>)</span>
NAME                           READY  STATUS   RESTARTS  AGE
test-mongodb-646b949fd4-xjxdb  0/1    Pending  <span class=m>0</span>         2s


NOTES:


** Please be patient <span class=k>while</span> the chart is being deployed **

MongoDB can be accessed via port <span class=m>27017</span> on the following DNS name from within your cluster:

    test-mongodb.default.svc.cluster.local

To get the root password run:

    <span class=nb>export</span> <span class=nv>MONGODB_ROOT_PASSWORD</span><span class=o>=</span><span class=k>$(</span>kubectl get secret --namespace default test-mongodb -o <span class=nv>jsonpath</span><span class=o>=</span><span class=s2>&#34;{.data.mongodb-root-password}&#34;</span> <span class=p>|</span> base64 --decode<span class=k>)</span>

To connect to your database run the following command:

    kubectl run --namespace default test-mongodb-client --rm --tty -i --restart<span class=o>=</span><span class=s1>&#39;Never&#39;</span> --image bitnami/mongodb --command -- mongo admin --host test-mongodb --authenticationDatabase admin -u root -p <span class=nv>$MONGODB_ROOT_PASSWORD</span>

To connect to your database from outside the cluster execute the following commands:

    kubectl port-forward --namespace default svc/test-mongodb 27017:27017 <span class=p>&amp;</span>
    mongo --host 127.0.0.1 --authenticationDatabase admin -p <span class=nv>$MONGODB_ROOT_PASSWORD</span>
</code></pre></div><p>Check to make sure the <code>PersistentVolume</code> and <code>PersistentVolumeClaim</code> deployed successfully, they should show a status of <code>Bound</code> if they have. (You may need to run this a few times while the volume provisions and gets mounted)</p>
<div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash>$ kubectl get pv,pvc
NAME                                                        CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS   CLAIM                  STORAGECLASS   REASON   AGE
persistentvolume/pvc-64941983-21b5-11e9-b851-005056b9750f   8Gi        RWO            Delete           Bound    default/test-mongodb   vsan-default            63m

NAME                                 STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS   AGE
persistentvolumeclaim/test-mongodb   Bound    pvc-64941983-21b5-11e9-b851-005056b9750f   8Gi        RWO            vsan-default   63m
</code></pre></div><p>Monitor the app&rsquo;s deployment and wait for all items to show as <code>Running</code></p>
<div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash>$ kubectl get all
NAME                                READY   STATUS    RESTARTS   AGE
pod/test-mongodb-646b949fd4-cz65g   1/1     Running   <span class=m>0</span>          62m

NAME                   TYPE        CLUSTER-IP      EXTERNAL-IP   PORT<span class=o>(</span>S<span class=o>)</span>     AGE
service/kubernetes     ClusterIP   10.96.0.1       &lt;none&gt;        443/TCP     3h20m
service/test-mongodb   ClusterIP   10.97.102.136   &lt;none&gt;        27017/TCP   62m

NAME                           READY   UP-TO-DATE   AVAILABLE   AGE
deployment.apps/test-mongodb   1/1     <span class=m>1</span>            <span class=m>1</span>           62m

NAME                                      DESIRED   CURRENT   READY   AGE
replicaset.apps/test-mongodb-646b949fd4   <span class=m>1</span>         <span class=m>1</span>         <span class=m>1</span>       62m
</code></pre></div><p>Verify the app works by testing access from the master node by spinning up a client container</p>
<div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=c1># Export the password set during deployment</span>
<span class=nb>export</span> <span class=nv>MONGODB_ROOT_PASSWORD</span><span class=o>=</span><span class=k>$(</span>kubectl get secret --namespace default test-mongodb -o <span class=nv>jsonpath</span><span class=o>=</span><span class=s2>&#34;{.data.mongodb-root-password}&#34;</span> <span class=p>|</span> base64 --decode<span class=k>)</span>

<span class=c1># Connect a client container to your server container</span>
kubectl run --namespace default test-mongodb-client --rm --tty -i --restart<span class=o>=</span><span class=s1>&#39;Never&#39;</span> --image bitnami/mongodb --command -- mongo admin --host test-mongodb --authenticationDatabase admin -u root -p <span class=nv>$MONGODB_ROOT_PASSWORD</span>

<span class=c1># Show databases</span>
&gt; show dbs
admin   0.000GB
config  0.000GB
<span class=nb>local</span>   0.000GB

&gt; <span class=nb>exit</span>
</code></pre></div><p>At this stage it is worth installing the kubernetes dashboard as well, for some visibility.</p>
<div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash>kubectl apply -f https://raw.githubusercontent.com/kubernetes/dashboard/v1.10.1/src/deploy/recommended/kubernetes-dashboard.yaml
</code></pre></div><p>Create a user (<code>ServiceAccount</code>) for accessing the UI with a new yaml file called <code>ui-user.yaml</code> (as you can see, I called mine <code>myles</code>)</p>
<div class=highlight><pre tabindex=0 class=chroma><code class=language-yaml data-lang=yaml><span class=nt>apiVersion</span><span class=p>:</span><span class=w> </span><span class=l>v1</span><span class=w>
</span><span class=w></span><span class=nt>kind</span><span class=p>:</span><span class=w> </span><span class=l>ServiceAccount</span><span class=w>
</span><span class=w></span><span class=nt>metadata</span><span class=p>:</span><span class=w>
</span><span class=w>  </span><span class=nt>name</span><span class=p>:</span><span class=w> </span><span class=l>myles</span><span class=w>
</span><span class=w>  </span><span class=nt>namespace</span><span class=p>:</span><span class=w> </span><span class=l>kube-system</span><span class=w>
</span></code></pre></div><p>Create a cluster-role for the user (<code>myles</code>) such that is has cluster-admin privileges - my file is called <code>clusterrolebinding.yaml</code></p>
<div class=highlight><pre tabindex=0 class=chroma><code class=language-yaml data-lang=yaml><span class=nt>apiVersion</span><span class=p>:</span><span class=w> </span><span class=l>rbac.authorization.k8s.io/v1</span><span class=w>
</span><span class=w></span><span class=nt>kind</span><span class=p>:</span><span class=w> </span><span class=l>ClusterRoleBinding</span><span class=w>
</span><span class=w></span><span class=nt>metadata</span><span class=p>:</span><span class=w>
</span><span class=w>  </span><span class=nt>name</span><span class=p>:</span><span class=w> </span><span class=l>myles</span><span class=w>
</span><span class=w></span><span class=nt>roleRef</span><span class=p>:</span><span class=w>
</span><span class=w>  </span><span class=nt>apiGroup</span><span class=p>:</span><span class=w> </span><span class=l>rbac.authorization.k8s.io</span><span class=w>
</span><span class=w>  </span><span class=nt>kind</span><span class=p>:</span><span class=w> </span><span class=l>ClusterRole</span><span class=w>
</span><span class=w>  </span><span class=nt>name</span><span class=p>:</span><span class=w> </span><span class=l>cluster-admin</span><span class=w>
</span><span class=w></span><span class=nt>subjects</span><span class=p>:</span><span class=w>
</span><span class=w></span>- <span class=nt>kind</span><span class=p>:</span><span class=w> </span><span class=l>ServiceAccount</span><span class=w>
</span><span class=w>  </span><span class=nt>name</span><span class=p>:</span><span class=w> </span><span class=l>myles</span><span class=w>
</span><span class=w>  </span><span class=nt>namespace</span><span class=p>:</span><span class=w> </span><span class=l>kube-system</span><span class=w>
</span></code></pre></div><p>Import both configs into K8s</p>
<div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash>kubectl create -f ui-user.yaml
kubectl create -f clusterrolebinding.yaml
</code></pre></div><p>Notice, we didn&rsquo;t create a password - they are generated automatically by K8S, so let&rsquo;s get the access token for the user we just created (change the <code>grep</code> section from <code>myles</code> in the command to reflect the username you used in <code>ui-user.yaml</code> above) - you will need to copy and paste the <code>token:</code> output from the command.</p>
<div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash>$ kubectl -n kube-system describe secret <span class=k>$(</span>kubectl -n kube-system get secret <span class=p>|</span> grep myles <span class=p>|</span> awk <span class=s1>&#39;{print $1}&#39;</span><span class=k>)</span>
Name:         myles-token-2mkr6
Namespace:    kube-system
Labels:       &lt;none&gt;
Annotations:  kubernetes.io/service-account.name: myles
              kubernetes.io/service-account.uid: 95920d01-21c9-11e9-b851-005056b9750f

Type:  kubernetes.io/service-account-token

<span class=nv>Data</span>
<span class=o>====</span>
token:      eyJhbGciOiJSUzI1NiIsImtpZCI6IiJ9.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJrdWJlLXN5c3RlbSIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VjcmV0Lm5hbWUiOiJteWxlcy10b2tlbi0ybWtyNiIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VydmljZS1hY2NvdW50Lm5hbWUiOiJteWxlcyIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VydmljZS1hY2NvdW50LnVpZCI6Ijk1OTIwZDAxLTIxYzktMTFlOS1iODUxLTAwNTA1NmI5NzUwZiIsInN1YiI6InN5c3RlbTpzZXJ2aWNlYWNjb3VudDprdWJlLXN5c3RlbTpteWxlcyJ9.YzVRG6Dt_p4_r7Uc7tCAEXjRG8xaB5HqeSO9DdcaXQWf6mqGhH2ahiXI3XdkqOm2725NHEJUsErD8GrJpGYnL_od15Zvxhn1D4VZr3Q3ds-nJ0IK2KS_ArXj3bypO6sjAEBb7bXviuWxge0bLlkurnuLYQSa9lrijHe95AGJnNrDi66Dr1eQoE4deJrjX7Bxm6ef2tikl6lCRA69Q57glQFBQm2aIvOUvR3y5b16vIVMQ6dJcnSE1EjB-G0n0lLRUzPij2nNU7IvmUBEzIbY2jFBxYiY8PEi0sMB2MZSitnW7DbMlZ5Yb8anHsX2XJbixl-VoDkMJuyujzwIc6vs8Q
ca.crt:     <span class=m>1025</span> bytes
namespace:  <span class=m>11</span> bytes
</code></pre></div><p>Create a proxy session to K8s to access the UI - note: by default, the dashboard is not accessible external to the cluster <a href=https://github.com/kubernetes/dashboard/wiki/Installation#recommended-setup>more detail on how to do that here</a></p>
<div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash>kubectl proxy <span class=p>&amp;</span>
</code></pre></div><p>Then access the dashboard (details on how to authenticate are <a href=https://github.com/kubernetes/dashboard/wiki/Access-control>available here</a>):</p>
<div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash>open http://localhost:8001/api/v1/namespaces/kube-system/services/https:kubernetes-dashboard:/proxy/
</code></pre></div><p>Change the access mode to <code>token</code> and paste in the output we copied above, profit</p>
<p><img loading=lazy src=images/Screenshot-2019-01-27-13.42.27.png alt="K8s dashboard set up">
</p>
<p>At this point you can browse around the K8s dashboard, view the <code>ReplicaSet</code> that <code>helm</code> created for mongodb, view the <code>PersistentVolume</code> created for us automatically from the <code>StorageClass</code> we defined and a lot more.</p>
<p>Feel free to have a look around, dive back into <code>helm</code> and deploy <a href=https://hub.helm.sh/charts/stable/rocketchat>some</a> <a href=https://hub.helm.sh/charts/stable/factorio>more</a> <a href=https://hub.helm.sh>apps</a>!</p>
<p>Why not follow <a href=https://twitter.com/mylesagray>@mylesagray on Twitter</a> for more like this!</p>
</div>
<footer class=post-footer>
<section class="section post-tags">
<div class=content>
<h2>Tagged with</h2>
</div>
<ul class=post-tags>
<li><a href=https://beta.blah.cloud/tags/containers/>containers</a></li>
<li><a href=https://beta.blah.cloud/tags/esxi/>esxi</a></li>
<li><a href=https://beta.blah.cloud/tags/helm/>helm</a></li>
<li><a href=https://beta.blah.cloud/tags/kubernetes/>kubernetes</a></li>
<li><a href=https://beta.blah.cloud/tags/vmware/>vmware</a></li>
<li><a href=https://beta.blah.cloud/tags/vsphere/>vsphere</a></li>
<li><a href=https://beta.blah.cloud/tags/persistent-storage/>persistent storage</a></li>
</ul>
</section>
<nav class=paginav>
<a class=prev href=https://beta.blah.cloud/infrastructure/using-cloud-init-for-vm-templating-on-vsphere/>
<span class=title>« Prev Page</span>
<br>
<span>Using cloud-init for VM templating on vSphere</span>
</a>
<a class=next href=https://beta.blah.cloud/kubernetes/setting-up-k8s-and-the-vsphere-cloud-provider-using-kubeadm/>
<span class=title>Next Page »</span>
<br>
<span>Setting up K8s and the vSphere Cloud Provider using kubeadm</span>
</a>
</nav>
<section class="section related-links">
<div class="columns is-centered">
<div class="column max-800px">
<div class=content>
<h2>Related content</h2>
</div>
<div class="columns related-links-columns">
<div class="column is-one-third">
<div class=card>
<div class=card-image>
<figure class="image is-3by2">
<a href=/kubernetes/first-look-automated-k8s-lifecycle-with-clusterapi/><img src=/kubernetes/first-look-automated-k8s-lifecycle-with-clusterapi/images/featured-image.png alt></a>
</figure>
</div>
<div class=card-content>
<a class="title is-5" href=https://beta.blah.cloud/kubernetes/first-look-automated-k8s-lifecycle-with-clusterapi/>First-look: Automated K8s lifecycle with ClusterAPI</a>
</div>
</div>
</div>
<div class="column is-one-third">
<div class=card>
<div class=card-image>
<figure class="image is-3by2">
<a href=/infrastructure/using-cloud-init-for-vm-templating-on-vsphere/><img src=/infrastructure/using-cloud-init-for-vm-templating-on-vsphere/images/Screenshot-2019-06-09-19.36.35.png alt></a>
</figure>
</div>
<div class=card-content>
<a class="title is-5" href=https://beta.blah.cloud/infrastructure/using-cloud-init-for-vm-templating-on-vsphere/>Using cloud-init for VM templating on vSphere</a>
</div>
</div>
</div>
<div class="column is-one-third">
<div class=card>
<div class=card-image>
<figure class="image is-3by2">
<a href=/kubernetes/setting-up-k8s-and-the-vsphere-cloud-provider-using-kubeadm/><img src=/kubernetes/setting-up-k8s-and-the-vsphere-cloud-provider-using-kubeadm/images/Screenshot-2019-01-28-00.55.21.png alt></a>
</figure>
</div>
<div class=card-content>
<a class="title is-5" href=https://beta.blah.cloud/kubernetes/setting-up-k8s-and-the-vsphere-cloud-provider-using-kubeadm/>Setting up K8s and the vSphere Cloud Provider using kubeadm</a>
</div>
</div>
</div>
</div>
</div>
</div>
</section>
<section class="section share-icons">
<div class=share-buttons>
<a target=_blank rel="noopener noreferrer" aria-label="share Using the vSphere Cloud Provider for K8s to dynamically deploy volumes on twitter" href="https://twitter.com/intent/tweet/?text=Using%20the%20vSphere%20Cloud%20Provider%20for%20K8s%20to%20dynamically%20deploy%20volumes&url=https%3a%2f%2fbeta.blah.cloud%2fkubernetes%2fusing-the-vsphere-cloud-provider-for-k8s-to-dynamically-deploy-volumes%2f&hashtags=containers%2cesxi%2chelm%2ckubernetes%2cvmware%2cvsphere%2cpersistentstorage"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM195.519 424.544c135.939.0 210.268-112.643 210.268-210.268.0-3.218.0-6.437-.153-9.502 14.406-10.421 26.973-23.448 36.935-38.314-13.18 5.824-27.433 9.809-42.452 11.648 15.326-9.196 26.973-23.602 32.49-40.92-14.252 8.429-30.038 14.56-46.896 17.931-13.487-14.406-32.644-23.295-53.946-23.295-40.767.0-73.87 33.104-73.87 73.87.0 5.824.613 11.494 1.992 16.858-61.456-3.065-115.862-32.49-152.337-77.241-6.284 10.881-9.962 23.601-9.962 37.088.0 25.594 13.027 48.276 32.95 61.456-12.107-.307-23.448-3.678-33.41-9.196v.92c0 35.862 25.441 65.594 59.311 72.49-6.13 1.686-12.72 2.606-19.464 2.606-4.751.0-9.348-.46-13.946-1.38 9.349 29.426 36.628 50.728 68.965 51.341-25.287 19.771-57.164 31.571-91.8 31.571-5.977.0-11.801-.306-17.625-1.073 32.337 21.15 71.264 33.41 112.95 33.41z"/></svg>
</a>
<a target=_blank rel="noopener noreferrer" aria-label="share Using the vSphere Cloud Provider for K8s to dynamically deploy volumes on reddit" href="https://reddit.com/submit?url=https%3a%2f%2fbeta.blah.cloud%2fkubernetes%2fusing-the-vsphere-cloud-provider-for-k8s-to-dynamically-deploy-volumes%2f&title=Using%20the%20vSphere%20Cloud%20Provider%20for%20K8s%20to%20dynamically%20deploy%20volumes"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg>
</a>
<a target=_blank rel="noopener noreferrer" aria-label="share Using the vSphere Cloud Provider for K8s to dynamically deploy volumes on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&url=https%3a%2f%2fbeta.blah.cloud%2fkubernetes%2fusing-the-vsphere-cloud-provider-for-k8s-to-dynamically-deploy-volumes%2f&title=Using%20the%20vSphere%20Cloud%20Provider%20for%20K8s%20to%20dynamically%20deploy%20volumes&summary=Using%20the%20vSphere%20Cloud%20Provider%20for%20K8s%20to%20dynamically%20deploy%20volumes&source=https%3a%2f%2fbeta.blah.cloud%2fkubernetes%2fusing-the-vsphere-cloud-provider-for-k8s-to-dynamically-deploy-volumes%2f"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg>
</a>
</div>
</section>
</footer><script src=https://utteranc.es/client.js repo=mylesagray/blog-comments issue-term=title theme=preferred-color-scheme crossorigin=anonymous async></script>
</article>
<aside class="hidden lg:block tableOfContentContainer" id=tableOfContentContainer>
<nav id=TableOfContents>
<ul>
<li><a href=#using-the-vcp>Using the VCP</a></li>
<li><a href=#prerequisites>Prerequisites</a>
<ul>
<li><a href=#tools>Tools</a></li>
</ul>
</li>
<li><a href=#vcenter>vCenter</a></li>
<li><a href=#kubernetes>Kubernetes</a></li>
</ul>
</nav>
</aside>
</div>
</main>
</div>
<footer class=footer>
<span>&copy; 2021 <a href=https://beta.blah.cloud/>Blah, Cloud</a></span>
<span>
Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://git.io/hugopapermod rel=noopener target=_blank>PaperMod</a>
</span>
</footer>
<a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a>
<script>let menu=document.getElementById('menu');menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)},document.querySelectorAll('a[href^="#"]').forEach(a=>{a.addEventListener("click",function(b){b.preventDefault();var a=this.getAttribute("href").substr(1);window.matchMedia('(prefers-reduced-motion: reduce)').matches?document.querySelector(`[id='${decodeURIComponent(a)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(a)}']`).scrollIntoView({behavior:"smooth"}),a==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${a}`)})})</script>
<script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script>
<script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove('dark'),localStorage.setItem("pref-theme",'light')):(document.body.classList.add('dark'),localStorage.setItem("pref-theme",'dark'))})</script>
<script>document.querySelectorAll('pre > code').forEach(b=>{const c=b.parentNode.parentNode,a=document.createElement('button');a.classList.add('copy-code'),a.innerText='copy';function d(){a.innerText='copied!',setTimeout(()=>{a.innerText='copy'},2e3)}a.addEventListener('click',e=>{if('clipboard'in navigator){navigator.clipboard.writeText(b.textContent),d();return}const a=document.createRange();a.selectNodeContents(b);const c=window.getSelection();c.removeAllRanges(),c.addRange(a);try{document.execCommand('copy'),d()}catch(a){}c.removeRange(a)}),c.classList.contains("highlight")?c.appendChild(a):c.parentNode.firstChild==c||(b.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?b.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(a):b.parentNode.appendChild(a))})</script>
</body>
</html>