<!doctype html><html lang=en dir=auto>
<head><meta charset=utf-8>
<meta http-equiv=x-ua-compatible content="IE=edge">
<meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no">
<meta name=robots content="index, follow">
<title>Setting up K8s and the vSphere Cloud Provider using kubeadm | Blah, Cloud</title>
<meta name=keywords content="cloud provider,kubeadm,kubernetes,linux,vmware,vsphere">
<meta name=description content="How to enable the vSphere Cloud Provider with kubeadm">
<meta name=author content="Myles Gray">
<link rel=canonical href=https://blah.cloud/amp/kubernetes/setting-up-k8s-and-the-vsphere-cloud-provider-using-kubeadm/>
<link crossorigin=anonymous href=/assets/css/stylesheet.min.e38a41c5e7d4d3ea02d394ee536b3395f1cb27bd5a9162de1e375c7d2d1bc8f0.css integrity="sha256-44pBxefU0+oC05TuU2szlfHLJ71akWLeHjdcfS0byPA=" rel="preload stylesheet" as=style>
<link rel=preload href=/images/logo-title.png as=image>
<script defer crossorigin=anonymous src=/assets/js/scroll-toc.min.a4bed116d37981ac4d9e6d35b2e80495aaddc43e6e6191cfd20f1d510a738b65.js integrity="sha256-pL7RFtN5gaxNnm01sugElardxD5uYZHP0g8dUQpzi2U="></script>
<link rel=icon href=https://blah.cloud/images/favicon.ico>
<link rel=icon type=image/png sizes=16x16 href=https://blah.cloud/images/favicon-16x16.png>
<link rel=icon type=image/png sizes=32x32 href=https://blah.cloud/images/favicon-32x32.png>
<link rel=apple-touch-icon href=https://blah.cloud/images/apple-touch-icon.png>
<link rel=mask-icon href=https://blah.cloud/images/logo.png>
<meta name=theme-color content="#2e2e33">
<meta name=msapplication-TileColor content="#2e2e33">
<meta name=generator content="Hugo 0.88.1">
<link rel=canonical type=text/html href=https://blah.cloud/kubernetes/setting-up-k8s-and-the-vsphere-cloud-provider-using-kubeadm/>
<meta property="og:title" content="Setting up K8s and the vSphere Cloud Provider using kubeadm">
<meta property="og:description" content="How to enable the vSphere Cloud Provider with kubeadm">
<meta property="og:type" content="article">
<meta property="og:url" content="https://blah.cloud/amp/kubernetes/setting-up-k8s-and-the-vsphere-cloud-provider-using-kubeadm/">
<meta property="og:image" content="https://blah.cloud/images/Screenshot-2019-01-28-00.55.21.png"><meta property="article:section" content="posts">
<meta property="article:published_time" content="2019-01-28T11:45:57+00:00">
<meta property="article:modified_time" content="2021-10-25T15:10:00+00:00"><meta property="og:site_name" content="Blah, Cloud">
<meta property="og:see_also" content="https://blah.cloud/amp/kubernetes/clusterapi-for-vsphere-now-with-cns-support/"><meta property="og:see_also" content="https://blah.cloud/amp/automation/using-velero-for-k8s-backup-and-restore-of-csi-volumes/"><meta property="og:see_also" content="https://blah.cloud/amp/kubernetes/first-look-automated-k8s-lifecycle-with-clusterapi/"><meta property="og:see_also" content="https://blah.cloud/amp/infrastructure/using-cloud-init-for-vm-templating-on-vsphere/"><meta property="og:see_also" content="https://blah.cloud/amp/kubernetes/using-the-vsphere-cloud-provider-for-k8s-to-dynamically-deploy-volumes/">
<meta name=twitter:card content="summary_large_image">
<meta name=twitter:image content="https://blah.cloud/images/Screenshot-2019-01-28-00.55.21.png">
<meta name=twitter:title content="Setting up K8s and the vSphere Cloud Provider using kubeadm">
<meta name=twitter:description content="How to enable the vSphere Cloud Provider with kubeadm">
<script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":2,"name":"Setting up K8s and the vSphere Cloud Provider using kubeadm","item":"https://blah.cloud/amp/kubernetes/setting-up-k8s-and-the-vsphere-cloud-provider-using-kubeadm/"}]}</script>
<script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Setting up K8s and the vSphere Cloud Provider using kubeadm","name":"Setting up K8s and the vSphere Cloud Provider using kubeadm","description":"How to enable the vSphere Cloud Provider with kubeadm","keywords":["cloud provider","kubeadm","kubernetes","linux","vmware","vsphere"],"articleBody":"Intro In the last installment we created an Ubuntu 18.04 LTS image to use to clone VMs from for spinning up our K8s nodes, we then cloned four VMs out, one as the master and three to be used as workers.\nThis time we are going to step through installing all the necessary K8s components on each of the nodes (kubeadm, kubectl and kubelet), the container runtime (Docker) and configuring the vSphere Cloud Provider for Kubernetes using kubeadm to bootstrap the cluster. We have a lot to cover, so let’s get to it!\nPrerequisites Tools I am using macOS, so will be using the brew package manager to install and manage my tools, if you are using Linux or Windows, use the appropriate install guide for each tool, according to your OS.\nFor each tool I will list the brew install command and the link to the install instructions for other OSes.\n brew  https://brew.sh   govc - brew tap govmomi/tap/govc \u0026\u0026 brew install govmomi/tap/govc  https://github.com/vmware/govmomi/tree/master/govc   kubectl - brew install kubernetes-cli  https://kubernetes.io/docs/tasks/tools/install-kubectl/   tmux (optional) - brew install tmux  https://github.com/tmux/tmux    Optional use of tmux If you want to speed things up and type the same commands to multiple sessions at once (there is going to be a lot or repetition otherwise), use tmux to open a SSH session to each of the IP addresses for your VMs (for more info see here)\ntmux new\\; split-window\\; split-window\\; split-window\\; select-layout even-vertical # Use ctrl b, then the arrow keys to cycle through the tmux panes and SSH to each box independently ssh ubuntu@vm.ip.address.here If you followed my tutuorial last time and all your boxes are named in the k8s* pattern, you can use the below command to get their IP addresses\ngovc find / -type m -name 'k8s*' | xargs govc vm.info | grep 'Name:\\|IP' Once you have SSH’d in to each box independently, you can turn on synchronisation\nctrl b, shift :, set synchronize-panes on I did up a quick asciinema to illustrate setup and use:\n\nSetting up VMs with K8s components On all nodes Install the container runtime (in our case Docker)\n# Install Docker CE # Update the apt package index sudo apt update ## Install packages to allow apt to use a repository over HTTPS sudo apt install ca-certificates software-properties-common apt-transport-https curl -y ## Add Docker’s official GPG key curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add - ## Add docker apt repository. sudo add-apt-repository \\ \"deb [arch=amd64] https://download.docker.com/linux/ubuntu \\ $(lsb_release -cs)\\ stable\" # Install docker ce (latest supported for K8s 1.13 is Docker 18.06) sudo apt update \u0026\u0026 sudo apt install docker-ce=18.06.1~ce~3-0~ubuntu -y # Setup daemon parameters, like log rotation and cgroups sudo tee /etc/docker/daemon.json /dev/null { \"exec-opts\": [\"native.cgroupdriver=systemd\"], \"log-driver\": \"json-file\", \"log-opts\": { \"max-size\": \"100m\" }, \"storage-driver\": \"overlay2\" } EOF sudo mkdir -p /etc/systemd/system/docker.service.d # Restart docker. sudo systemctl daemon-reload sudo systemctl restart docker Install the K8s components\n# Add the K8s repo to apt curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key add - echo \"deb https://apt.kubernetes.io/ kubernetes-xenial main\" | sudo tee /etc/apt/sources.list.d/kubernetes.list /dev/null # Install kubelet, kubectl and kubeadm for cluster spinup sudo apt update sudo apt install kubelet kubeadm kubectl -y # Hold K8s packages at their installed version so as not to upgrade unexpectedly on an apt upgrade sudo apt-mark hold kubelet kubeadm kubectl We will be using flannel for pod networking in this example, so the below needs to be run on all nodes to pass bridged IPv4 traffic to iptables chains:\nsudo sysctl net.bridge.bridge-nf-call-iptables=1 Enabling the VMware vSphere Cloud Provider On the master(s) Create your vsphere.conf file with vCenter details Edit the below command to fill in your vCenter details before running.\nIf you don’t have a folder created with your kubernetes node VMs added we can do that quickly with govc (note, change vSAN-DC to your Datacenter name in vCenter):\ngovc folder.create /vSAN-DC/vm/k8s govc object.mv /vSAN-DC/vm/k8s-\\* /vSAN-DC/vm/k8s Details on syntax can be found here. It is important to note, whatever VM folder you specify below needs to be pre-created in your vCenter, in my case the folder is called k8s.\nsudo tee /etc/kubernetes/vsphere.conf /dev/null [Global] user = \"administrator@vsphere.local\" password = \"Admin!23\" port = \"443\" insecure-flag = \"1\" [VirtualCenter \"10.198.17.154\"] datacenters = \"vSAN-DC\" [Workspace] server = \"10.198.17.154\" datacenter = \"vSAN-DC\" default-datastore = \"vsanDatastore\" resourcepool-path = \"vSAN-Cluster/Resources\" folder = \"k8s\" [Disk] scsicontrollertype = pvscsi [Network] public-network = \"VM Network\" EOF Activate the vSphere Cloud Provider in our kubeadm init config file. Additionally, as we are deploying flannel as our overlay network for pods and it requires the below subnet CIDR in order for the overlay to work.\nsudo tee /etc/kubernetes/kubeadminitmaster.yaml /dev/null apiVersion:kubeadm.k8s.io/v1beta1kind:InitConfigurationbootstrapTokens:- groups:- system:bootstrappers:kubeadm:default-node-tokentoken:y7yaev.9dvwxx6ny4ef8vlqttl:0susages:- signing- authenticationnodeRegistration:kubeletExtraArgs:cloud-provider:\"vsphere\"cloud-config:\"/etc/kubernetes/vsphere.conf\"---apiVersion:kubeadm.k8s.io/v1beta1kind:ClusterConfigurationkubernetesVersion:v1.13.3apiServer:extraArgs:cloud-provider:\"vsphere\"cloud-config:\"/etc/kubernetes/vsphere.conf\"extraVolumes:- name:cloudhostPath:\"/etc/kubernetes/vsphere.conf\"mountPath:\"/etc/kubernetes/vsphere.conf\"controllerManager:extraArgs:cloud-provider:\"vsphere\"cloud-config:\"/etc/kubernetes/vsphere.conf\"extraVolumes:- name:cloudhostPath:\"/etc/kubernetes/vsphere.conf\"mountPath:\"/etc/kubernetes/vsphere.conf\"networking:podSubnet:\"10.244.0.0/16\"EOFRestart the kubelet daemon to reload the configuration\nsudo systemctl daemon-reload sudo systemctl restart kubelet Initialising the cluster with kubeadm On all nodes Firstly, verify that connectivity to the required gcr.io registries is working by pulling the containers required by kubeadm\n$ sudo kubeadm config images pull [config/images] Pulled k8s.gcr.io/kube-apiserver:v1.13.2 [config/images] Pulled k8s.gcr.io/kube-controller-manager:v1.13.2 [config/images] Pulled k8s.gcr.io/kube-scheduler:v1.13.2 [config/images] Pulled k8s.gcr.io/kube-proxy:v1.13.2 [config/images] Pulled k8s.gcr.io/pause:3.1 [config/images] Pulled k8s.gcr.io/etcd:3.2.24 [config/images] Pulled k8s.gcr.io/coredns:1.2.6 On the master node(s) Initialise kubeadm with the config file from above which includes our vSphere Cloud Provider and Flannel CIDR configurations.\n$ sudo kubeadm init --config /etc/kubernetes/kubeadminitmaster.yaml [init] Using Kubernetes version: v1.13.0 [preflight] Running pre-flight checks [preflight] Pulling images required for setting up a Kubernetes cluster [preflight] This might take a minute or two, depending on the speed of your internet connection [preflight] You can also perform this action in beforehand using 'kubeadm config images pull' [kubelet-start] Writing kubelet environment file with flags to file \"/var/lib/kubelet/kubeadm-flags.env\" [kubelet-start] Writing kubelet configuration to file \"/var/lib/kubelet/config.yaml\" [kubelet-start] Activating the kubelet service [certs] Using certificateDir folder \"/etc/kubernetes/pki\" [certs] Generating \"etcd/ca\" certificate and key [certs] Generating \"etcd/peer\" certificate and key [certs] etcd/peer serving cert is signed for DNS names [k8s-master localhost] and IPs [10.198.17.177 127.0.0.1 ::1] [certs] Generating \"etcd/healthcheck-client\" certificate and key [certs] Generating \"apiserver-etcd-client\" certificate and key [certs] Generating \"etcd/server\" certificate and key [certs] etcd/server serving cert is signed for DNS names [k8s-master localhost] and IPs [10.198.17.177 127.0.0.1 ::1] [certs] Generating \"ca\" certificate and key [certs] Generating \"apiserver\" certificate and key [certs] apiserver serving cert is signed for DNS names [k8s-master kubernetes kubernetes.default kubernetes.default.svc kubernetes.default.svc.cluster.local] and IPs [10.96.0.1 10.198.17.177] [certs] Generating \"apiserver-kubelet-client\" certificate and key [certs] Generating \"front-proxy-ca\" certificate and key [certs] Generating \"front-proxy-client\" certificate and key [certs] Generating \"sa\" key and public key [kubeconfig] Using kubeconfig folder \"/etc/kubernetes\" [kubeconfig] Writing \"admin.conf\" kubeconfig file [kubeconfig] Writing \"kubelet.conf\" kubeconfig file [kubeconfig] Writing \"controller-manager.conf\" kubeconfig file [kubeconfig] Writing \"scheduler.conf\" kubeconfig file [control-plane] Using manifest folder \"/etc/kubernetes/manifests\" [control-plane] Creating static Pod manifest for \"kube-apiserver\" [controlplane] Adding extra host path mount \"cloud\" to \"kube-apiserver\" [controlplane] Adding extra host path mount \"cloud\" to \"kube-controller-manager\" [control-plane] Creating static Pod manifest for \"kube-controller-manager\" [controlplane] Adding extra host path mount \"cloud\" to \"kube-apiserver\" [controlplane] Adding extra host path mount \"cloud\" to \"kube-controller-manager\" [control-plane] Creating static Pod manifest for \"kube-scheduler\" [controlplane] Adding extra host path mount \"cloud\" to \"kube-apiserver\" [controlplane] Adding extra host path mount \"cloud\" to \"kube-controller-manager\" [etcd] Creating static Pod manifest for local etcd in \"/etc/kubernetes/manifests\" [wait-control-plane] Waiting for the kubelet to boot up the control plane as static Pods from directory \"/etc/kubernetes/manifests\". This can take up to 4m0s [apiclient] All control plane components are healthy after 23.503056 seconds [uploadconfig] storing the configuration used in ConfigMap \"kubeadm-config\" in the \"kube-system\" Namespace [kubelet] Creating a ConfigMap \"kubelet-config-1.13\" in namespace kube-system with the configuration for the kubelets in the cluster [patchnode] Uploading the CRI Socket information \"/var/run/dockershim.sock\" to the Node API object \"k8s-master\" as an annotation [mark-control-plane] Marking the node k8s-master as control-plane by adding the label \"node-role.kubernetes.io/master=''\" [mark-control-plane] Marking the node k8s-master as control-plane by adding the taints [node-role.kubernetes.io/master:NoSchedule] [bootstrap-token] Using token: p8iv6v.zu8eofjtbc9r54dd [bootstrap-token] Configuring bootstrap tokens, cluster-info ConfigMap, RBAC Roles [bootstraptoken] configured RBAC rules to allow Node Bootstrap tokens to post CSRs in order for nodes to get long term certificate credentials [bootstraptoken] configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token [bootstraptoken] configured RBAC rules to allow certificate rotation for all node client certificates in the cluster [bootstraptoken] creating the \"cluster-info\" ConfigMap in the \"kube-public\" namespace [addons] Applied essential addon: CoreDNS [addons] Applied essential addon: kube-proxy Your Kubernetes master has initialized successfully! To start using your cluster, you need to run the following as a regular user: mkdir -p $HOME/.kube sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config sudo chown $(id -u):$(id -g) $HOME/.kube/config You should now deploy a pod network to the cluster. Run \"kubectl apply -f [podnetwork].yaml\" with one of the options listed at: https://kubernetes.io/docs/concepts/cluster-administration/addons/ You can now join any number of machines by running the following on each node as root: kubeadm join 10.198.17.177:6443 --token p8iv6v.zu8eofjtbc9r54dd --discovery-token-ca-cert-hash sha256:398f667fb3a6ffe6296e4d07c825834b54cce73bacf58641915cf79a1d1895f7 A lot of text will output as it spins up the cluster components, if all is successful, we can start using the cluster now by importing the kubeconfig.\nmkdir -p $HOME/.kube sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config sudo chown $(id -u):$(id -g) $HOME/.kube/config You can also use it on external systems by copying the output from the below command into your local computer’s ~/.kube/config file:\nsudo cat /etc/kubernetes/admin.conf Let’s deploy our flannel pod overlay networking so the pods can communicate with each other.\nkubectl apply -f https://raw.githubusercontent.com/coreos/flannel/bc79dd1505b0c8681ece4de4c0d86c5cd2643275/Documentation/kube-flannel.yml Check to make sure the pods are all in the status Running:\n$ kubectl get pods --all-namespaces NAMESPACE NAME READY STATUS RESTARTS AGE kube-system coredns-86c58d9df4-fqbdm 1/1 Running 0 2m19s kube-system coredns-86c58d9df4-zhpj6 1/1 Running 0 2m19s kube-system etcd-k8s-master 1/1 Running 0 2m37s kube-system kube-apiserver-k8s-master 1/1 Running 0 68s kube-system kube-controller-manager-k8s-master 1/1 Running 0 2m36s kube-system kube-flannel-ds-amd64-8cst6 1/1 Running 0 26s kube-system kube-proxy-6grkv 1/1 Running 0 2m19s kube-system kube-scheduler-k8s-master 1/1 Running 0 2m36s Export the master node config used to point the workers being joined to the master:\nkubectl -n kube-public get configmap cluster-info -o jsonpath='{.data.kubeconfig}'  discovery.yaml On your laptop Copy the discovery.yaml to your local machine with scp.\nscp ubuntu@10.198.17.177:~/discovery.yaml discovery.yaml Then upload it to the worker nodes.\nscp discovery.yaml ubuntu@10.198.17.189:~/discovery.yaml scp discovery.yaml ubuntu@10.198.17.190:~/discovery.yaml scp discovery.yaml ubuntu@10.198.17.191:~/discovery.yaml On the worker nodes To check and make sure the discovery.yaml file was copied correctly, do a quick cat.\ncat ~/discovery.yaml Then create the worker node kubeadm config yaml file (notice it’s using our discovery.yaml as the input for master discovery) and the token is the same as we put in the master kubeadminitmaster.yaml configuration above and we specify the cloud-provider as vsphere for the workers:\nsudo tee /etc/kubernetes/kubeadminitworker.yaml /dev/null apiVersion:kubeadm.k8s.io/v1alpha3kind:JoinConfigurationdiscoveryFile:discovery.yamltoken:y7yaev.9dvwxx6ny4ef8vlqnodeRegistration:kubeletExtraArgs:cloud-provider:vsphereEOFAnd now we should be able to join our workers to the cluster.\n$ sudo kubeadm join --config /etc/kubernetes/kubeadminitworker.yaml [preflight] Running pre-flight checks [discovery] Trying to connect to API Server \"10.198.17.177:6443\" [discovery] Created cluster-info discovery client, requesting info from \"https://10.198.17.177:6443\" [discovery] Requesting info from \"https://10.198.17.177:6443\" again to validate TLS against the pinned public key [discovery] Cluster info signature and contents are valid and TLS certificate validates against pinned roots, will use API Server \"10.198.17.177:6443\" [discovery] Successfully established connection with API Server \"10.198.17.177:6443\" [join] Reading configuration from the cluster... [join] FYI: You can look at this config file with 'kubectl -n kube-system get cm kubeadm-config -oyaml' [kubelet] Downloading configuration for the kubelet from the \"kubelet-config-1.13\" ConfigMap in the kube-system namespace [kubelet-start] Writing kubelet configuration to file \"/var/lib/kubelet/config.yaml\" [kubelet-start] Writing kubelet environment file with flags to file \"/var/lib/kubelet/kubeadm-flags.env\" [kubelet-start] Activating the kubelet service [tlsbootstrap] Waiting for the kubelet to perform the TLS Bootstrap... [patchnode] Uploading the CRI Socket information \"/var/run/dockershim.sock\" to the Node API object \"k8s-worker1\" as an annotation This node has joined the cluster: *Certificate signing request was sent to apiserver and a response was received. * The Kubelet was informed of the new secure connection details. Run 'kubectl get nodes' on the master to see this node join the cluster. Verify setup Now, as the output says above, back on the master check that all nodes have joined the cluster\nubuntu@k8s-master:~$ kubectl get nodes -o wide NAME STATUS ROLES AGE VERSION INTERNAL-IP EXTERNAL-IP OS-IMAGE KERNEL-VERSION CONTAINER-RUNTIME k8s-master Ready master 4m44s v1.13.2 10.198.17.177 10.198.17.177 Ubuntu 18.04.1 LTS 4.15.0-43-generic docker://18.6.0 k8s-worker1 Ready  33s v1.13.2 10.198.17.174  Ubuntu 18.04.1 LTS 4.15.0-43-generic docker://18.6.0 k8s-worker2 Ready  32s v1.13.2 10.198.17.175  Ubuntu 18.04.1 LTS 4.15.0-43-generic docker://18.6.0 k8s-worker3 Ready  32s v1.13.2 10.198.17.176  Ubuntu 18.04.1 LTS 4.15.0-43-generic docker://18.6.0 Verify the providerID is set on all the nodes for the VCP to operate correctly:\nubuntu@k8s-master:~$ kubectl describe nodes | grep \"ProviderID\" ProviderID: vsphere://420f0d85-cf4a-c7a7-e52d-18e9b4b71dec ProviderID: vsphere://420fc2b2-64ab-a477-f7b1-37d4e6747abf ProviderID: vsphere://420f2d75-37bd-8b56-4e2f-421cbcbbb0b2 ProviderID: vsphere://420f7ec3-2dbd-601e-240b-4ee6d8945210 We now have a fully up and running k8s cluster with the vSphere Cloud Provider installed! Check out part 3 where we install the K8s dashboard and show how the integration with the vSphere Cloud Provider really works!\nWhy not follow @mylesagray on Twitter for more like this!\n","wordCount":"2100","inLanguage":"en","image":"https://blah.cloud/images/Screenshot-2019-01-28-00.55.21.png","datePublished":"2019-01-28T11:45:57Z","dateModified":"2021-10-25T15:10:00Z","author":{"@type":"Person","name":"Myles Gray"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://blah.cloud/amp/kubernetes/setting-up-k8s-and-the-vsphere-cloud-provider-using-kubeadm/"},"publisher":{"@type":"Organization","name":"Blah, Cloud","logo":{"@type":"ImageObject","url":"https://blah.cloud/images/favicon.ico"}}}</script>
<script defer data-domain=blah.cloud src=https://plausible.io/js/plausible.js></script>
<script>window.plausible=window.plausible||function(){(window.plausible.q=window.plausible.q||[]).push(arguments)}</script>
</head>
<body id=top>
<script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add('dark'):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove('dark'):window.matchMedia('(prefers-color-scheme: dark)').matches&&document.body.classList.add('dark')</script>
<script>var themeColor=getComputedStyle(document.body).getPropertyValue('--primary');document.querySelector('meta[name="theme-color"]').setAttribute('content',themeColor)</script>
<noscript>
<style type=text/css>#theme-toggle,.top-link{display:none}</style>
<style>@media(prefers-color-scheme:dark){:root{--theme:#1d1e20;--entry:#2e2e33;--primary:rgba(240, 202, 102, 1);--secondary:rgba(216, 214, 197, 1);--tertiary:rgba(128, 130 ,133 , 1);--content:rgba(206, 205, 188, 1);--hljs-bg:#282a36;--code-bg:#282a36;--border:rgba(240, 202, 102, 1)}.list{background:var(--theme)}}</style>
</noscript>
<header class=header>
<nav class=nav>
<div class=logo>
<a href=https://blah.cloud/ accesskey=h title="Blah, Cloud. (Alt + H)">
<img src=/images/logo-title.png alt=logo aria-label=logo height=40></a>
</div>
<span class=logo-switches>
<button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg>
</button>
</span>
<ul id=menu>
<li>
<a href=https://blah.cloud/blog/ title=blog>
<span>blog</span>
</a>
</li>
<li>
<a href=https://blah.cloud/search/ title=" (Alt + /)" accesskey=/>
<span><svg style="height:1em;margin-top:1.5em" aria-hidden="true" focusable="false" data-prefix="fas" data-icon="search" class="fill-current w-5" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path d="M505 442.7 405.3 343c-4.5-4.5-10.6-7-17-7H372c27.6-35.3 44-79.7 44-128C416 93.1 322.9.0 208 0S0 93.1.0 208s93.1 208 208 208c48.3.0 92.7-16.4 128-44v16.3c0 6.4 2.5 12.5 7 17l99.7 99.7c9.4 9.4 24.6 9.4 33.9.0l28.3-28.3c9.4-9.4 9.4-24.6.1-34zM208 336c-70.7.0-128-57.2-128-128 0-70.7 57.2-128 128-128 70.7.0 128 57.2 128 128 0 70.7-57.2 128-128 128z"/></svg></span>
</a>
</li>
</ul>
</nav>
</header>
<div class=container>
<main class=main>
<div class=wrapper>
<article class=post-single>
<header class=post-header>
<h1 class=post-title>
Setting up K8s and the vSphere Cloud Provider using kubeadm
</h1>
<div class=post-description>
How to enable the vSphere Cloud Provider with kubeadm
</div>
<div class=post-meta>January 28, 2019&nbsp;·&nbsp;Myles Gray&nbsp;|&nbsp;<a href=https://github.com/mylesagray/blog/blob/master/content/posts/2019-01-28-setting-up-k8s-and-the-vsphere-cloud-provider-using-kubeadm/index.md rel="noopener noreferrer" target=_blank>Suggest Changes</a>
</div>
</header>
<figure class=entry-cover>
<img loading=lazy srcset="https://blah.cloud/amp/kubernetes/setting-up-k8s-and-the-vsphere-cloud-provider-using-kubeadm/images/Screenshot-2019-01-28-00.55.21_hu9b4da0a5ccf334dc21f765c580b1ca2a_76297_360x0_resize_box_3.png 360w ,https://blah.cloud/amp/kubernetes/setting-up-k8s-and-the-vsphere-cloud-provider-using-kubeadm/images/Screenshot-2019-01-28-00.55.21_hu9b4da0a5ccf334dc21f765c580b1ca2a_76297_480x0_resize_box_3.png 480w ,https://blah.cloud/amp/kubernetes/setting-up-k8s-and-the-vsphere-cloud-provider-using-kubeadm/images/Screenshot-2019-01-28-00.55.21_hu9b4da0a5ccf334dc21f765c580b1ca2a_76297_720x0_resize_box_3.png 720w ,https://blah.cloud/amp/kubernetes/setting-up-k8s-and-the-vsphere-cloud-provider-using-kubeadm/images/Screenshot-2019-01-28-00.55.21_hu9b4da0a5ccf334dc21f765c580b1ca2a_76297_1080x0_resize_box_3.png 1080w ,https://blah.cloud/amp/kubernetes/setting-up-k8s-and-the-vsphere-cloud-provider-using-kubeadm/images/Screenshot-2019-01-28-00.55.21.png 1141w" sizes="(min-width: 768px) 720px, 100vw" src=https://blah.cloud/amp/kubernetes/setting-up-k8s-and-the-vsphere-cloud-provider-using-kubeadm/images/Screenshot-2019-01-28-00.55.21.png alt="Running K8s cluster with VCP" width=1141 height=418>
</figure><div class=toc>
<details>
<summary accesskey=c title="(Alt + C)">
<span class=details>Table of Contents</span>
</summary>
<div class=inner><ul>
<li>
<a href=#intro aria-label=Intro>Intro</a></li>
<li>
<a href=#prerequisites aria-label=Prerequisites>Prerequisites</a><ul>
<li>
<a href=#tools aria-label=Tools>Tools</a></li>
<li>
<a href=#optional-use-of-tmux aria-label="Optional use of tmux">Optional use of tmux</a></li></ul>
</li>
<li>
<a href=#setting-up-vms-with-k8s-components aria-label="Setting up VMs with K8s components">Setting up VMs with K8s components</a><ul>
<li>
<a href=#on-all-nodes aria-label="On all nodes">On all nodes</a></li></ul>
</li>
<li>
<a href=#enabling-the-vmware-vsphere-cloud-provider aria-label="Enabling the VMware vSphere Cloud Provider">Enabling the VMware vSphere Cloud Provider</a><ul>
<li>
<a href=#on-the-masters aria-label="On the master(s)">On the master(s)</a><ul>
<li>
<a href=#create-your-vsphereconf-file-with-vcenter-details aria-label="Create your vsphere.conf file with vCenter details">Create your <code>vsphere.conf</code> file with vCenter details</a></li></ul>
</li></ul>
</li>
<li>
<a href=#initialising-the-cluster-with-kubeadm aria-label="Initialising the cluster with kubeadm">Initialising the cluster with kubeadm</a><ul>
<li>
<a href=#on-all-nodes-1 aria-label="On all nodes">On all nodes</a></li>
<li>
<a href=#on-the-master-nodes aria-label="On the master node(s)">On the master node(s)</a></li>
<li>
<a href=#on-your-laptop aria-label="On your laptop">On your laptop</a></li>
<li>
<a href=#on-the-worker-nodes aria-label="On the worker nodes">On the worker nodes</a></li></ul>
</li>
<li>
<a href=#verify-setup aria-label="Verify setup">Verify setup</a>
</li>
</ul>
</div>
</details>
</div>
<div class=post-content><h2 id=intro>Intro<a hidden class=anchor aria-hidden=true href=#intro>#</a></h2>
<p>In the <a href=/kubernetes/creating-an-ubuntu-18-04-lts-cloud-image-for-cloning-on-vmware/>last installment</a> we created an Ubuntu 18.04 LTS image to use to clone VMs from for spinning up our K8s nodes, we then cloned four VMs out, one as the master and three to be used as workers.</p>
<p>This time we are going to step through installing all the necessary K8s components on each of the nodes (<code>kubeadm</code>, <code>kubectl</code> and <code>kubelet</code>), the container runtime (Docker) and configuring the vSphere Cloud Provider for Kubernetes using <code>kubeadm</code> to bootstrap the cluster. We have a lot to cover, so let&rsquo;s get to it!</p>
<h2 id=prerequisites>Prerequisites<a hidden class=anchor aria-hidden=true href=#prerequisites>#</a></h2>
<h3 id=tools>Tools<a hidden class=anchor aria-hidden=true href=#tools>#</a></h3>
<p>I am using macOS, so will be using the <code>brew</code> package manager to install and manage my tools, if you are using Linux or Windows, use the appropriate install guide for each tool, according to your OS.</p>
<p>For each tool I will list the <code>brew</code> install command and the link to the install instructions for other OSes.</p>
<ul>
<li>brew
<ul>
<li><a href=https://brew.sh>https://brew.sh</a></li>
</ul>
</li>
<li>govc - <code>brew tap govmomi/tap/govc && brew install govmomi/tap/govc</code>
<ul>
<li><a href=https://github.com/vmware/govmomi/tree/master/govc>https://github.com/vmware/govmomi/tree/master/govc</a></li>
</ul>
</li>
<li>kubectl - <code>brew install kubernetes-cli</code>
<ul>
<li><a href=https://kubernetes.io/docs/tasks/tools/install-kubectl/>https://kubernetes.io/docs/tasks/tools/install-kubectl/</a></li>
</ul>
</li>
<li>tmux (optional) - <code>brew install tmux</code>
<ul>
<li><a href=https://github.com/tmux/tmux>https://github.com/tmux/tmux</a></li>
</ul>
</li>
</ul>
<h3 id=optional-use-of-tmux>Optional use of tmux<a hidden class=anchor aria-hidden=true href=#optional-use-of-tmux>#</a></h3>
<p>If you want to speed things up and type the same commands to multiple sessions at once (there is going to be a lot or repetition otherwise), use <code>tmux</code> to open a SSH session to each of the IP addresses for your VMs (for more info see <a href=https://hackernoon.com/a-gentle-introduction-to-tmux-8d784c404340>here</a>)</p>
<div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash>tmux new<span class=se>\;</span> split-window<span class=se>\;</span> split-window<span class=se>\;</span> split-window<span class=se>\;</span> <span class=k>select</span>-layout even-vertical
<span class=c1># Use ctrl b, then the arrow keys to cycle through the tmux panes and SSH to each box independently</span>
ssh ubuntu@vm.ip.address.here
</code></pre></div><p>If you followed my tutuorial last time and all your boxes are named in the <code>k8s*</code> pattern, you can use the below command to get their IP addresses</p>
<div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash>govc find / -type m -name <span class=s1>&#39;k8s*&#39;</span> <span class=p>|</span> xargs govc vm.info <span class=p>|</span> grep <span class=s1>&#39;Name:\|IP&#39;</span>
</code></pre></div><p>Once you have SSH&rsquo;d in to each box independently, you can turn on synchronisation</p>
<div class=highlight><pre tabindex=0 class=chroma><code class=language-sh data-lang=sh>ctrl b, <span class=nb>shift</span> :, <span class=nb>set</span> synchronize-panes on
</code></pre></div><p>I did up a quick <code>asciinema</code> to <a href=https://asciinema.org/a/223790>illustrate setup and use</a>:</p>
<p><a href=https://asciinema.org/a/223790><img loading=lazy src=https://asciinema.org/a/223790.svg alt=asciicast>
</a></p>
<h2 id=setting-up-vms-with-k8s-components>Setting up VMs with K8s components<a hidden class=anchor aria-hidden=true href=#setting-up-vms-with-k8s-components>#</a></h2>
<h3 id=on-all-nodes>On all nodes<a hidden class=anchor aria-hidden=true href=#on-all-nodes>#</a></h3>
<p>Install the container runtime (in our case Docker)</p>
<div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=c1># Install Docker CE</span>
<span class=c1># Update the apt package index</span>
sudo apt update

<span class=c1>## Install packages to allow apt to use a repository over HTTPS</span>
sudo apt install ca-certificates software-properties-common apt-transport-https curl -y

<span class=c1>## Add Docker’s official GPG key</span>
curl -fsSL https://download.docker.com/linux/ubuntu/gpg <span class=p>|</span> sudo apt-key add -

<span class=c1>## Add docker apt repository.</span>
sudo add-apt-repository <span class=se>\
</span><span class=se></span><span class=s2>&#34;deb [arch=amd64] https://download.docker.com/linux/ubuntu \
</span><span class=s2></span><span class=k>$(</span>lsb_release -cs<span class=k>)</span><span class=s2> \
</span><span class=s2>stable&#34;</span>

<span class=c1># Install docker ce (latest supported for K8s 1.13 is Docker 18.06)</span>
sudo apt update <span class=o>&amp;&amp;</span> sudo apt install docker-ce<span class=o>=</span>18.06.1~ce~3-0~ubuntu -y

<span class=c1># Setup daemon parameters, like log rotation and cgroups</span>
sudo tee /etc/docker/daemon.json &gt;/dev/null <span class=s>&lt;&lt;EOF
</span><span class=s>{
</span><span class=s>  &#34;exec-opts&#34;: [&#34;native.cgroupdriver=systemd&#34;],
</span><span class=s>  &#34;log-driver&#34;: &#34;json-file&#34;,
</span><span class=s>  &#34;log-opts&#34;: {
</span><span class=s>    &#34;max-size&#34;: &#34;100m&#34;
</span><span class=s>  },
</span><span class=s>  &#34;storage-driver&#34;: &#34;overlay2&#34;
</span><span class=s>}
</span><span class=s>EOF</span>

sudo mkdir -p /etc/systemd/system/docker.service.d

<span class=c1># Restart docker.</span>
sudo systemctl daemon-reload
sudo systemctl restart docker
</code></pre></div><p>Install the K8s components</p>
<div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=c1># Add the K8s repo to apt</span>
curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg <span class=p>|</span> sudo apt-key add -
<span class=nb>echo</span> <span class=s2>&#34;deb https://apt.kubernetes.io/ kubernetes-xenial main&#34;</span> <span class=p>|</span> sudo tee /etc/apt/sources.list.d/kubernetes.list &gt;/dev/null

<span class=c1># Install kubelet, kubectl and kubeadm for cluster spinup</span>
sudo apt update
sudo apt install kubelet kubeadm kubectl -y

<span class=c1># Hold K8s packages at their installed version so as not to upgrade unexpectedly on an apt upgrade</span>
sudo apt-mark hold kubelet kubeadm kubectl
</code></pre></div><p>We will be using <a href=https://github.com/coreos/flannel><code>flannel</code></a> for pod networking in this example, so the below needs to be run on all nodes to pass <a href=https://kubernetes.io/docs/setup/independent/create-cluster-kubeadm/#tabs-pod-install-4>bridged IPv4 traffic to iptables chains</a>:</p>
<div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash>sudo sysctl net.bridge.bridge-nf-call-iptables<span class=o>=</span><span class=m>1</span>
</code></pre></div><h2 id=enabling-the-vmware-vsphere-cloud-provider>Enabling the VMware vSphere Cloud Provider<a hidden class=anchor aria-hidden=true href=#enabling-the-vmware-vsphere-cloud-provider>#</a></h2>
<h3 id=on-the-masters>On the master(s)<a hidden class=anchor aria-hidden=true href=#on-the-masters>#</a></h3>
<h4 id=create-your-vsphereconf-file-with-vcenter-details>Create your <code>vsphere.conf</code> file with vCenter details<a hidden class=anchor aria-hidden=true href=#create-your-vsphereconf-file-with-vcenter-details>#</a></h4>
<p>Edit the below command to fill in your vCenter details before running.</p>
<p>If you don&rsquo;t have a folder created with your kubernetes node VMs added we can do that quickly with <code>govc</code> (note, change <code>vSAN-DC</code> to your Datacenter name in vCenter):</p>
<div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash>govc folder.create /vSAN-DC/vm/k8s
govc object.mv /vSAN-DC/vm/k8s-<span class=se>\*</span> /vSAN-DC/vm/k8s
</code></pre></div><p>Details on <a href=https://vmware.github.io/vsphere-storage-for-kubernetes/documentation/existing.html>syntax can be found here</a>. It is important to note, whatever VM folder you specify below needs to be pre-created in your vCenter, in my case the folder is called <code>k8s</code>.</p>
<div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash>sudo tee /etc/kubernetes/vsphere.conf &gt;/dev/null <span class=s>&lt;&lt;EOF
</span><span class=s>[Global]
</span><span class=s>user = &#34;administrator@vsphere.local&#34;
</span><span class=s>password = &#34;Admin!23&#34;
</span><span class=s>port = &#34;443&#34;
</span><span class=s>insecure-flag = &#34;1&#34;
</span><span class=s>
</span><span class=s>[VirtualCenter &#34;10.198.17.154&#34;]
</span><span class=s>datacenters = &#34;vSAN-DC&#34;
</span><span class=s>
</span><span class=s>[Workspace]
</span><span class=s>server = &#34;10.198.17.154&#34;
</span><span class=s>datacenter = &#34;vSAN-DC&#34;
</span><span class=s>default-datastore = &#34;vsanDatastore&#34;
</span><span class=s>resourcepool-path = &#34;vSAN-Cluster/Resources&#34;
</span><span class=s>folder = &#34;k8s&#34;
</span><span class=s>
</span><span class=s>[Disk]
</span><span class=s>scsicontrollertype = pvscsi
</span><span class=s>
</span><span class=s>[Network]
</span><span class=s>public-network = &#34;VM Network&#34;
</span><span class=s>EOF</span>
</code></pre></div><p>Activate the vSphere Cloud Provider in our <code>kubeadm init</code> config file. Additionally, as we are deploying <code>flannel</code> as our overlay network for pods and it requires the below subnet CIDR in order for the overlay to work.</p>
<div class=highlight><pre tabindex=0 class=chroma><code class=language-yaml data-lang=yaml><span class=l>sudo tee /etc/kubernetes/kubeadminitmaster.yaml &gt;/dev/null &lt;&lt;EOF</span><span class=w>
</span><span class=w></span><span class=nt>apiVersion</span><span class=p>:</span><span class=w> </span><span class=l>kubeadm.k8s.io/v1beta1</span><span class=w>
</span><span class=w></span><span class=nt>kind</span><span class=p>:</span><span class=w> </span><span class=l>InitConfiguration</span><span class=w>
</span><span class=w></span><span class=nt>bootstrapTokens</span><span class=p>:</span><span class=w>
</span><span class=w>       </span>- <span class=nt>groups</span><span class=p>:</span><span class=w>
</span><span class=w>         </span>- <span class=l>system:bootstrappers:kubeadm:default-node-token</span><span class=w>
</span><span class=w>         </span><span class=nt>token</span><span class=p>:</span><span class=w> </span><span class=l>y7yaev.9dvwxx6ny4ef8vlq</span><span class=w>
</span><span class=w>         </span><span class=nt>ttl</span><span class=p>:</span><span class=w> </span><span class=l>0s</span><span class=w>
</span><span class=w>         </span><span class=nt>usages</span><span class=p>:</span><span class=w>
</span><span class=w>         </span>- <span class=l>signing</span><span class=w>
</span><span class=w>         </span>- <span class=l>authentication</span><span class=w>
</span><span class=w></span><span class=nt>nodeRegistration</span><span class=p>:</span><span class=w>
</span><span class=w>  </span><span class=nt>kubeletExtraArgs</span><span class=p>:</span><span class=w>
</span><span class=w>    </span><span class=nt>cloud-provider</span><span class=p>:</span><span class=w> </span><span class=s2>&#34;vsphere&#34;</span><span class=w>
</span><span class=w>    </span><span class=nt>cloud-config</span><span class=p>:</span><span class=w> </span><span class=s2>&#34;/etc/kubernetes/vsphere.conf&#34;</span><span class=w>
</span><span class=w></span><span class=nn>---</span><span class=w>
</span><span class=w></span><span class=nt>apiVersion</span><span class=p>:</span><span class=w> </span><span class=l>kubeadm.k8s.io/v1beta1</span><span class=w>
</span><span class=w></span><span class=nt>kind</span><span class=p>:</span><span class=w> </span><span class=l>ClusterConfiguration</span><span class=w>
</span><span class=w></span><span class=nt>kubernetesVersion</span><span class=p>:</span><span class=w> </span><span class=l>v1.13.3</span><span class=w>
</span><span class=w></span><span class=nt>apiServer</span><span class=p>:</span><span class=w>
</span><span class=w>  </span><span class=nt>extraArgs</span><span class=p>:</span><span class=w>
</span><span class=w>    </span><span class=nt>cloud-provider</span><span class=p>:</span><span class=w> </span><span class=s2>&#34;vsphere&#34;</span><span class=w>
</span><span class=w>    </span><span class=nt>cloud-config</span><span class=p>:</span><span class=w> </span><span class=s2>&#34;/etc/kubernetes/vsphere.conf&#34;</span><span class=w>
</span><span class=w>  </span><span class=nt>extraVolumes</span><span class=p>:</span><span class=w>
</span><span class=w>  </span>- <span class=nt>name</span><span class=p>:</span><span class=w> </span><span class=l>cloud</span><span class=w>
</span><span class=w>    </span><span class=nt>hostPath</span><span class=p>:</span><span class=w> </span><span class=s2>&#34;/etc/kubernetes/vsphere.conf&#34;</span><span class=w>
</span><span class=w>    </span><span class=nt>mountPath</span><span class=p>:</span><span class=w> </span><span class=s2>&#34;/etc/kubernetes/vsphere.conf&#34;</span><span class=w>
</span><span class=w></span><span class=nt>controllerManager</span><span class=p>:</span><span class=w>
</span><span class=w>  </span><span class=nt>extraArgs</span><span class=p>:</span><span class=w>
</span><span class=w>    </span><span class=nt>cloud-provider</span><span class=p>:</span><span class=w> </span><span class=s2>&#34;vsphere&#34;</span><span class=w>
</span><span class=w>    </span><span class=nt>cloud-config</span><span class=p>:</span><span class=w> </span><span class=s2>&#34;/etc/kubernetes/vsphere.conf&#34;</span><span class=w>
</span><span class=w>  </span><span class=nt>extraVolumes</span><span class=p>:</span><span class=w>
</span><span class=w>  </span>- <span class=nt>name</span><span class=p>:</span><span class=w> </span><span class=l>cloud</span><span class=w>
</span><span class=w>    </span><span class=nt>hostPath</span><span class=p>:</span><span class=w> </span><span class=s2>&#34;/etc/kubernetes/vsphere.conf&#34;</span><span class=w>
</span><span class=w>    </span><span class=nt>mountPath</span><span class=p>:</span><span class=w> </span><span class=s2>&#34;/etc/kubernetes/vsphere.conf&#34;</span><span class=w>
</span><span class=w></span><span class=nt>networking</span><span class=p>:</span><span class=w>
</span><span class=w>  </span><span class=nt>podSubnet</span><span class=p>:</span><span class=w> </span><span class=s2>&#34;10.244.0.0/16&#34;</span><span class=w>
</span><span class=w></span><span class=l>EOF</span><span class=w>
</span></code></pre></div><p>Restart the kubelet daemon to reload the configuration</p>
<div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash>sudo systemctl daemon-reload
sudo systemctl restart kubelet
</code></pre></div><h2 id=initialising-the-cluster-with-kubeadm>Initialising the cluster with kubeadm<a hidden class=anchor aria-hidden=true href=#initialising-the-cluster-with-kubeadm>#</a></h2>
<h3 id=on-all-nodes-1>On all nodes<a hidden class=anchor aria-hidden=true href=#on-all-nodes-1>#</a></h3>
<p>Firstly, verify that connectivity to the required <code>gcr.io</code> registries is working by pulling the containers required by <code>kubeadm</code></p>
<div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash>$ sudo kubeadm config images pull
<span class=o>[</span>config/images<span class=o>]</span> Pulled k8s.gcr.io/kube-apiserver:v1.13.2
<span class=o>[</span>config/images<span class=o>]</span> Pulled k8s.gcr.io/kube-controller-manager:v1.13.2
<span class=o>[</span>config/images<span class=o>]</span> Pulled k8s.gcr.io/kube-scheduler:v1.13.2
<span class=o>[</span>config/images<span class=o>]</span> Pulled k8s.gcr.io/kube-proxy:v1.13.2
<span class=o>[</span>config/images<span class=o>]</span> Pulled k8s.gcr.io/pause:3.1
<span class=o>[</span>config/images<span class=o>]</span> Pulled k8s.gcr.io/etcd:3.2.24
<span class=o>[</span>config/images<span class=o>]</span> Pulled k8s.gcr.io/coredns:1.2.6
</code></pre></div><h3 id=on-the-master-nodes>On the master node(s)<a hidden class=anchor aria-hidden=true href=#on-the-master-nodes>#</a></h3>
<p>Initialise <code>kubeadm</code> with the config file from above which includes our vSphere Cloud Provider and Flannel CIDR configurations.</p>
<div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash>$ sudo kubeadm init --config /etc/kubernetes/kubeadminitmaster.yaml
<span class=o>[</span>init<span class=o>]</span> Using Kubernetes version: v1.13.0
<span class=o>[</span>preflight<span class=o>]</span> Running pre-flight checks
<span class=o>[</span>preflight<span class=o>]</span> Pulling images required <span class=k>for</span> setting up a Kubernetes cluster
<span class=o>[</span>preflight<span class=o>]</span> This might take a minute or two, depending on the speed of your internet connection
<span class=o>[</span>preflight<span class=o>]</span> You can also perform this action in beforehand using <span class=s1>&#39;kubeadm config images pull&#39;</span>
<span class=o>[</span>kubelet-start<span class=o>]</span> Writing kubelet environment file with flags to file <span class=s2>&#34;/var/lib/kubelet/kubeadm-flags.env&#34;</span>
<span class=o>[</span>kubelet-start<span class=o>]</span> Writing kubelet configuration to file <span class=s2>&#34;/var/lib/kubelet/config.yaml&#34;</span>
<span class=o>[</span>kubelet-start<span class=o>]</span> Activating the kubelet service
<span class=o>[</span>certs<span class=o>]</span> Using certificateDir folder <span class=s2>&#34;/etc/kubernetes/pki&#34;</span>
<span class=o>[</span>certs<span class=o>]</span> Generating <span class=s2>&#34;etcd/ca&#34;</span> certificate and key
<span class=o>[</span>certs<span class=o>]</span> Generating <span class=s2>&#34;etcd/peer&#34;</span> certificate and key
<span class=o>[</span>certs<span class=o>]</span> etcd/peer serving cert is signed <span class=k>for</span> DNS names <span class=o>[</span>k8s-master localhost<span class=o>]</span> and IPs <span class=o>[</span>10.198.17.177 127.0.0.1 ::1<span class=o>]</span>
<span class=o>[</span>certs<span class=o>]</span> Generating <span class=s2>&#34;etcd/healthcheck-client&#34;</span> certificate and key
<span class=o>[</span>certs<span class=o>]</span> Generating <span class=s2>&#34;apiserver-etcd-client&#34;</span> certificate and key
<span class=o>[</span>certs<span class=o>]</span> Generating <span class=s2>&#34;etcd/server&#34;</span> certificate and key
<span class=o>[</span>certs<span class=o>]</span> etcd/server serving cert is signed <span class=k>for</span> DNS names <span class=o>[</span>k8s-master localhost<span class=o>]</span> and IPs <span class=o>[</span>10.198.17.177 127.0.0.1 ::1<span class=o>]</span>
<span class=o>[</span>certs<span class=o>]</span> Generating <span class=s2>&#34;ca&#34;</span> certificate and key
<span class=o>[</span>certs<span class=o>]</span> Generating <span class=s2>&#34;apiserver&#34;</span> certificate and key
<span class=o>[</span>certs<span class=o>]</span> apiserver serving cert is signed <span class=k>for</span> DNS names <span class=o>[</span>k8s-master kubernetes kubernetes.default kubernetes.default.svc kubernetes.default.svc.cluster.local<span class=o>]</span> and IPs <span class=o>[</span>10.96.0.1 10.198.17.177<span class=o>]</span>
<span class=o>[</span>certs<span class=o>]</span> Generating <span class=s2>&#34;apiserver-kubelet-client&#34;</span> certificate and key
<span class=o>[</span>certs<span class=o>]</span> Generating <span class=s2>&#34;front-proxy-ca&#34;</span> certificate and key
<span class=o>[</span>certs<span class=o>]</span> Generating <span class=s2>&#34;front-proxy-client&#34;</span> certificate and key
<span class=o>[</span>certs<span class=o>]</span> Generating <span class=s2>&#34;sa&#34;</span> key and public key
<span class=o>[</span>kubeconfig<span class=o>]</span> Using kubeconfig folder <span class=s2>&#34;/etc/kubernetes&#34;</span>
<span class=o>[</span>kubeconfig<span class=o>]</span> Writing <span class=s2>&#34;admin.conf&#34;</span> kubeconfig file
<span class=o>[</span>kubeconfig<span class=o>]</span> Writing <span class=s2>&#34;kubelet.conf&#34;</span> kubeconfig file
<span class=o>[</span>kubeconfig<span class=o>]</span> Writing <span class=s2>&#34;controller-manager.conf&#34;</span> kubeconfig file
<span class=o>[</span>kubeconfig<span class=o>]</span> Writing <span class=s2>&#34;scheduler.conf&#34;</span> kubeconfig file
<span class=o>[</span>control-plane<span class=o>]</span> Using manifest folder <span class=s2>&#34;/etc/kubernetes/manifests&#34;</span>
<span class=o>[</span>control-plane<span class=o>]</span> Creating static Pod manifest <span class=k>for</span> <span class=s2>&#34;kube-apiserver&#34;</span>
<span class=o>[</span>controlplane<span class=o>]</span> Adding extra host path mount <span class=s2>&#34;cloud&#34;</span> to <span class=s2>&#34;kube-apiserver&#34;</span>
<span class=o>[</span>controlplane<span class=o>]</span> Adding extra host path mount <span class=s2>&#34;cloud&#34;</span> to <span class=s2>&#34;kube-controller-manager&#34;</span>
<span class=o>[</span>control-plane<span class=o>]</span> Creating static Pod manifest <span class=k>for</span> <span class=s2>&#34;kube-controller-manager&#34;</span>
<span class=o>[</span>controlplane<span class=o>]</span> Adding extra host path mount <span class=s2>&#34;cloud&#34;</span> to <span class=s2>&#34;kube-apiserver&#34;</span>
<span class=o>[</span>controlplane<span class=o>]</span> Adding extra host path mount <span class=s2>&#34;cloud&#34;</span> to <span class=s2>&#34;kube-controller-manager&#34;</span>
<span class=o>[</span>control-plane<span class=o>]</span> Creating static Pod manifest <span class=k>for</span> <span class=s2>&#34;kube-scheduler&#34;</span>
<span class=o>[</span>controlplane<span class=o>]</span> Adding extra host path mount <span class=s2>&#34;cloud&#34;</span> to <span class=s2>&#34;kube-apiserver&#34;</span>
<span class=o>[</span>controlplane<span class=o>]</span> Adding extra host path mount <span class=s2>&#34;cloud&#34;</span> to <span class=s2>&#34;kube-controller-manager&#34;</span>
<span class=o>[</span>etcd<span class=o>]</span> Creating static Pod manifest <span class=k>for</span> <span class=nb>local</span> etcd in <span class=s2>&#34;/etc/kubernetes/manifests&#34;</span>
<span class=o>[</span>wait-control-plane<span class=o>]</span> Waiting <span class=k>for</span> the kubelet to boot up the control plane as static Pods from directory <span class=s2>&#34;/etc/kubernetes/manifests&#34;</span>. This can take up to 4m0s
<span class=o>[</span>apiclient<span class=o>]</span> All control plane components are healthy after 23.503056 seconds
<span class=o>[</span>uploadconfig<span class=o>]</span> storing the configuration used in ConfigMap <span class=s2>&#34;kubeadm-config&#34;</span> in the <span class=s2>&#34;kube-system&#34;</span> Namespace
<span class=o>[</span>kubelet<span class=o>]</span> Creating a ConfigMap <span class=s2>&#34;kubelet-config-1.13&#34;</span> in namespace kube-system with the configuration <span class=k>for</span> the kubelets in the cluster
<span class=o>[</span>patchnode<span class=o>]</span> Uploading the CRI Socket information <span class=s2>&#34;/var/run/dockershim.sock&#34;</span> to the Node API object <span class=s2>&#34;k8s-master&#34;</span> as an annotation
<span class=o>[</span>mark-control-plane<span class=o>]</span> Marking the node k8s-master as control-plane by adding the label <span class=s2>&#34;node-role.kubernetes.io/master=&#39;&#39;&#34;</span>
<span class=o>[</span>mark-control-plane<span class=o>]</span> Marking the node k8s-master as control-plane by adding the taints <span class=o>[</span>node-role.kubernetes.io/master:NoSchedule<span class=o>]</span>
<span class=o>[</span>bootstrap-token<span class=o>]</span> Using token: p8iv6v.zu8eofjtbc9r54dd
<span class=o>[</span>bootstrap-token<span class=o>]</span> Configuring bootstrap tokens, cluster-info ConfigMap, RBAC Roles
<span class=o>[</span>bootstraptoken<span class=o>]</span> configured RBAC rules to allow Node Bootstrap tokens to post CSRs in order <span class=k>for</span> nodes to get long term certificate credentials
<span class=o>[</span>bootstraptoken<span class=o>]</span> configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token
<span class=o>[</span>bootstraptoken<span class=o>]</span> configured RBAC rules to allow certificate rotation <span class=k>for</span> all node client certificates in the cluster
<span class=o>[</span>bootstraptoken<span class=o>]</span> creating the <span class=s2>&#34;cluster-info&#34;</span> ConfigMap in the <span class=s2>&#34;kube-public&#34;</span> namespace
<span class=o>[</span>addons<span class=o>]</span> Applied essential addon: CoreDNS
<span class=o>[</span>addons<span class=o>]</span> Applied essential addon: kube-proxy

Your Kubernetes master has initialized successfully!

To start using your cluster, you need to run the following as a regular user:

  mkdir -p <span class=nv>$HOME</span>/.kube
  sudo cp -i /etc/kubernetes/admin.conf <span class=nv>$HOME</span>/.kube/config
  sudo chown <span class=k>$(</span>id -u<span class=k>)</span>:<span class=k>$(</span>id -g<span class=k>)</span> <span class=nv>$HOME</span>/.kube/config

You should now deploy a pod network to the cluster.
Run <span class=s2>&#34;kubectl apply -f [podnetwork].yaml&#34;</span> with one of the options listed at:
  https://kubernetes.io/docs/concepts/cluster-administration/addons/

You can now join any number of machines by running the following on each node
as root:

  kubeadm join 10.198.17.177:6443 --token p8iv6v.zu8eofjtbc9r54dd --discovery-token-ca-cert-hash sha256:398f667fb3a6ffe6296e4d07c825834b54cce73bacf58641915cf79a1d1895f7
</code></pre></div><p>A lot of text will output as it spins up the cluster components, if all is successful, we can start using the cluster now by importing the <code>kubeconfig</code>.</p>
<div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash>mkdir -p <span class=nv>$HOME</span>/.kube
sudo cp -i /etc/kubernetes/admin.conf <span class=nv>$HOME</span>/.kube/config
sudo chown <span class=k>$(</span>id -u<span class=k>)</span>:<span class=k>$(</span>id -g<span class=k>)</span> <span class=nv>$HOME</span>/.kube/config
</code></pre></div><p>You can also use it on external systems by copying the output from the below command into your local computer&rsquo;s <code>~/.kube/config</code> file:</p>
<div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash>sudo cat /etc/kubernetes/admin.conf
</code></pre></div><p>Let&rsquo;s deploy our <code>flannel</code> pod overlay networking so the pods can communicate with each other.</p>
<div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash>kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/bc79dd1505b0c8681ece4de4c0d86c5cd2643275/Documentation/kube-flannel.yml
</code></pre></div><p>Check to make sure the pods are all in the status <code>Running</code>:</p>
<div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash>$ kubectl get pods --all-namespaces
NAMESPACE     NAME                                 READY   STATUS    RESTARTS   AGE
kube-system   coredns-86c58d9df4-fqbdm             1/1     Running   <span class=m>0</span>          2m19s
kube-system   coredns-86c58d9df4-zhpj6             1/1     Running   <span class=m>0</span>          2m19s
kube-system   etcd-k8s-master                      1/1     Running   <span class=m>0</span>          2m37s
kube-system   kube-apiserver-k8s-master            1/1     Running   <span class=m>0</span>          68s
kube-system   kube-controller-manager-k8s-master   1/1     Running   <span class=m>0</span>          2m36s
kube-system   kube-flannel-ds-amd64-8cst6          1/1     Running   <span class=m>0</span>          26s
kube-system   kube-proxy-6grkv                     1/1     Running   <span class=m>0</span>          2m19s
kube-system   kube-scheduler-k8s-master            1/1     Running   <span class=m>0</span>          2m36s
</code></pre></div><p>Export the master node config used to point the workers being joined to the master:</p>
<div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash>kubectl -n kube-public get configmap cluster-info -o <span class=nv>jsonpath</span><span class=o>=</span><span class=s1>&#39;{.data.kubeconfig}&#39;</span> &gt; discovery.yaml
</code></pre></div><h3 id=on-your-laptop>On your laptop<a hidden class=anchor aria-hidden=true href=#on-your-laptop>#</a></h3>
<p>Copy the <code>discovery.yaml</code> to your local machine with <code>scp</code>.</p>
<div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash>scp ubuntu@10.198.17.177:~/discovery.yaml discovery.yaml
</code></pre></div><p>Then upload it to the worker nodes.</p>
<div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash>scp discovery.yaml ubuntu@10.198.17.189:~/discovery.yaml
scp discovery.yaml ubuntu@10.198.17.190:~/discovery.yaml
scp discovery.yaml ubuntu@10.198.17.191:~/discovery.yaml
</code></pre></div><h3 id=on-the-worker-nodes>On the worker nodes<a hidden class=anchor aria-hidden=true href=#on-the-worker-nodes>#</a></h3>
<p>To check and make sure the <code>discovery.yaml</code> file was copied correctly, do a quick <code>cat</code>.</p>
<div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash>cat ~/discovery.yaml
</code></pre></div><p>Then create the worker node <code>kubeadm</code> config yaml file (notice it&rsquo;s using our <code>discovery.yaml</code> as the input for master discovery) and the <code>token</code> is the same as we put in the master <code>kubeadminitmaster.yaml</code> configuration above and we specify the <code>cloud-provider</code> as <code>vsphere</code> for the workers:</p>
<div class=highlight><pre tabindex=0 class=chroma><code class=language-yaml data-lang=yaml><span class=l>sudo tee /etc/kubernetes/kubeadminitworker.yaml &gt;/dev/null &lt;&lt;EOF</span><span class=w>
</span><span class=w></span><span class=nt>apiVersion</span><span class=p>:</span><span class=w> </span><span class=l>kubeadm.k8s.io/v1alpha3</span><span class=w>
</span><span class=w></span><span class=nt>kind</span><span class=p>:</span><span class=w> </span><span class=l>JoinConfiguration</span><span class=w>
</span><span class=w></span><span class=nt>discoveryFile</span><span class=p>:</span><span class=w> </span><span class=l>discovery.yaml</span><span class=w>
</span><span class=w></span><span class=nt>token</span><span class=p>:</span><span class=w> </span><span class=l>y7yaev.9dvwxx6ny4ef8vlq</span><span class=w>
</span><span class=w></span><span class=nt>nodeRegistration</span><span class=p>:</span><span class=w>
</span><span class=w>  </span><span class=nt>kubeletExtraArgs</span><span class=p>:</span><span class=w>
</span><span class=w>    </span><span class=nt>cloud-provider</span><span class=p>:</span><span class=w> </span><span class=l>vsphere</span><span class=w>
</span><span class=w></span><span class=l>EOF</span><span class=w>
</span></code></pre></div><p>And now we should be able to join our workers to the cluster.</p>
<div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash>$ sudo kubeadm join --config /etc/kubernetes/kubeadminitworker.yaml

<span class=o>[</span>preflight<span class=o>]</span> Running pre-flight checks
<span class=o>[</span>discovery<span class=o>]</span> Trying to connect to API Server <span class=s2>&#34;10.198.17.177:6443&#34;</span>
<span class=o>[</span>discovery<span class=o>]</span> Created cluster-info discovery client, requesting info from <span class=s2>&#34;https://10.198.17.177:6443&#34;</span>
<span class=o>[</span>discovery<span class=o>]</span> Requesting info from <span class=s2>&#34;https://10.198.17.177:6443&#34;</span> again to validate TLS against the pinned public key
<span class=o>[</span>discovery<span class=o>]</span> Cluster info signature and contents are valid and TLS certificate validates against pinned roots, will use API Server <span class=s2>&#34;10.198.17.177:6443&#34;</span>
<span class=o>[</span>discovery<span class=o>]</span> Successfully established connection with API Server <span class=s2>&#34;10.198.17.177:6443&#34;</span>
<span class=o>[</span>join<span class=o>]</span> Reading configuration from the cluster...
<span class=o>[</span>join<span class=o>]</span> FYI: You can look at this config file with <span class=s1>&#39;kubectl -n kube-system get cm kubeadm-config -oyaml&#39;</span>
<span class=o>[</span>kubelet<span class=o>]</span> Downloading configuration <span class=k>for</span> the kubelet from the <span class=s2>&#34;kubelet-config-1.13&#34;</span> ConfigMap in the kube-system namespace
<span class=o>[</span>kubelet-start<span class=o>]</span> Writing kubelet configuration to file <span class=s2>&#34;/var/lib/kubelet/config.yaml&#34;</span>
<span class=o>[</span>kubelet-start<span class=o>]</span> Writing kubelet environment file with flags to file <span class=s2>&#34;/var/lib/kubelet/kubeadm-flags.env&#34;</span>
<span class=o>[</span>kubelet-start<span class=o>]</span> Activating the kubelet service
<span class=o>[</span>tlsbootstrap<span class=o>]</span> Waiting <span class=k>for</span> the kubelet to perform the TLS Bootstrap...
<span class=o>[</span>patchnode<span class=o>]</span> Uploading the CRI Socket information <span class=s2>&#34;/var/run/dockershim.sock&#34;</span> to the Node API object <span class=s2>&#34;k8s-worker1&#34;</span> as an annotation

This node has joined the cluster:
*Certificate signing request was sent to apiserver and a response was received.
* The Kubelet was informed of the new secure connection details.

Run <span class=s1>&#39;kubectl get nodes&#39;</span> on the master to see this node join the cluster.
</code></pre></div><h2 id=verify-setup>Verify setup<a hidden class=anchor aria-hidden=true href=#verify-setup>#</a></h2>
<p>Now, as the output says above, back on the master check that all nodes have joined the cluster</p>
<div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash>ubuntu@k8s-master:~$ kubectl get nodes -o wide
NAME          STATUS   ROLES    AGE     VERSION   INTERNAL-IP     EXTERNAL-IP     OS-IMAGE             KERNEL-VERSION      CONTAINER-RUNTIME
k8s-master    Ready    master   4m44s   v1.13.2   10.198.17.177   10.198.17.177   Ubuntu 18.04.1 LTS   4.15.0-43-generic   docker://18.6.0
k8s-worker1   Ready    &lt;none&gt;   33s     v1.13.2   10.198.17.174   &lt;none&gt;          Ubuntu 18.04.1 LTS   4.15.0-43-generic   docker://18.6.0
k8s-worker2   Ready    &lt;none&gt;   32s     v1.13.2   10.198.17.175   &lt;none&gt;          Ubuntu 18.04.1 LTS   4.15.0-43-generic   docker://18.6.0
k8s-worker3   Ready    &lt;none&gt;   32s     v1.13.2   10.198.17.176   &lt;none&gt;          Ubuntu 18.04.1 LTS   4.15.0-43-generic   docker://18.6.0
</code></pre></div><p>Verify the <code>providerID</code> is set on all the nodes for the VCP to operate correctly:</p>
<div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash>ubuntu@k8s-master:~$ kubectl describe nodes <span class=p>|</span> grep <span class=s2>&#34;ProviderID&#34;</span>
ProviderID:                  vsphere://420f0d85-cf4a-c7a7-e52d-18e9b4b71dec
ProviderID:                  vsphere://420fc2b2-64ab-a477-f7b1-37d4e6747abf
ProviderID:                  vsphere://420f2d75-37bd-8b56-4e2f-421cbcbbb0b2
ProviderID:                  vsphere://420f7ec3-2dbd-601e-240b-4ee6d8945210
</code></pre></div><p>We now have a fully up and running k8s cluster with the vSphere Cloud Provider installed! Check out <a href=/kubernetes/using-the-vsphere-cloud-provider-for-k8s-to-dynamically-deploy-volumes/>part 3 where we install</a> the K8s dashboard and show how the integration with the vSphere Cloud Provider really works!</p>
<p>Why not follow <a href=https://twitter.com/mylesagray>@mylesagray on Twitter</a> for more like this!</p>
</div>
<footer class=post-footer>
<section class="section post-tags">
<div class=content>
<h2>Tagged with</h2>
</div>
<ul class=post-tags>
<li><a href=https://blah.cloud/tags/cloud-provider/>cloud provider</a></li>
<li><a href=https://blah.cloud/tags/kubeadm/>kubeadm</a></li>
<li><a href=https://blah.cloud/tags/kubernetes/>kubernetes</a></li>
<li><a href=https://blah.cloud/tags/linux/>linux</a></li>
<li><a href=https://blah.cloud/tags/vmware/>vmware</a></li>
<li><a href=https://blah.cloud/tags/vsphere/>vsphere</a></li>
</ul>
</section>
<nav class=paginav>
<a class=prev href=https://blah.cloud/amp/kubernetes/using-the-vsphere-cloud-provider-for-k8s-to-dynamically-deploy-volumes/>
<span class=title>« Prev Page</span>
<br>
<span>Using the vSphere Cloud Provider for K8s to dynamically deploy volumes</span>
</a>
<a class=next href=https://blah.cloud/amp/kubernetes/creating-an-ubuntu-18-04-lts-cloud-image-for-cloning-on-vmware/>
<span class=title>Next Page »</span>
<br>
<span>Creating an Ubuntu 18.04 LTS cloud image for cloning on VMware</span>
</a>
</nav>
<section class="section related-links">
<div class="columns is-centered">
<div class="column max-800px">
<div class=content>
<h2>Related content</h2>
</div>
<div class="columns related-links-columns">
<div class="column is-one-third">
<div class=card>
<div class=card-image>
<figure class="image is-3by2">
<a href=/amp/kubernetes/first-look-automated-k8s-lifecycle-with-clusterapi/><img src=/amp/kubernetes/first-look-automated-k8s-lifecycle-with-clusterapi/images/featured-image.png alt></a>
</figure>
</div>
<div class=card-content>
<a class="title is-5" href=https://blah.cloud/amp/kubernetes/first-look-automated-k8s-lifecycle-with-clusterapi/>First-look: Automated K8s lifecycle with ClusterAPI</a>
</div>
</div>
</div>
<div class="column is-one-third">
<div class=card>
<div class=card-image>
<figure class="image is-3by2">
<a href=/amp/kubernetes/creating-an-ubuntu-18-04-lts-cloud-image-for-cloning-on-vmware/><img src=/amp/kubernetes/creating-an-ubuntu-18-04-lts-cloud-image-for-cloning-on-vmware/images/Screenshot-2019-01-27-22.07.55.png alt></a>
</figure>
</div>
<div class=card-content>
<a class="title is-5" href=https://blah.cloud/amp/kubernetes/creating-an-ubuntu-18-04-lts-cloud-image-for-cloning-on-vmware/>Creating an Ubuntu 18.04 LTS cloud image for cloning on VMware</a>
</div>
</div>
</div>
<div class="column is-one-third">
<div class=card>
<div class=card-image>
<figure class="image is-3by2">
<a href=/amp/infrastructure/using-cloud-init-for-vm-templating-on-vsphere/><img src=/amp/infrastructure/using-cloud-init-for-vm-templating-on-vsphere/images/Screenshot-2019-06-09-19.36.35.png alt></a>
</figure>
</div>
<div class=card-content>
<a class="title is-5" href=https://blah.cloud/amp/infrastructure/using-cloud-init-for-vm-templating-on-vsphere/>Using cloud-init for VM templating on vSphere</a>
</div>
</div>
</div>
</div>
</div>
</div>
</section>
<section class="section share-icons">
<div class=share-buttons>
<a target=_blank rel="noopener noreferrer" aria-label="share Setting up K8s and the vSphere Cloud Provider using kubeadm on twitter" href="https://twitter.com/intent/tweet/?text=Setting%20up%20K8s%20and%20the%20vSphere%20Cloud%20Provider%20using%20kubeadm&url=https%3a%2f%2fblah.cloud%2famp%2fkubernetes%2fsetting-up-k8s-and-the-vsphere-cloud-provider-using-kubeadm%2f&hashtags=cloudprovider%2ckubeadm%2ckubernetes%2clinux%2cvmware%2cvsphere"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM195.519 424.544c135.939.0 210.268-112.643 210.268-210.268.0-3.218.0-6.437-.153-9.502 14.406-10.421 26.973-23.448 36.935-38.314-13.18 5.824-27.433 9.809-42.452 11.648 15.326-9.196 26.973-23.602 32.49-40.92-14.252 8.429-30.038 14.56-46.896 17.931-13.487-14.406-32.644-23.295-53.946-23.295-40.767.0-73.87 33.104-73.87 73.87.0 5.824.613 11.494 1.992 16.858-61.456-3.065-115.862-32.49-152.337-77.241-6.284 10.881-9.962 23.601-9.962 37.088.0 25.594 13.027 48.276 32.95 61.456-12.107-.307-23.448-3.678-33.41-9.196v.92c0 35.862 25.441 65.594 59.311 72.49-6.13 1.686-12.72 2.606-19.464 2.606-4.751.0-9.348-.46-13.946-1.38 9.349 29.426 36.628 50.728 68.965 51.341-25.287 19.771-57.164 31.571-91.8 31.571-5.977.0-11.801-.306-17.625-1.073 32.337 21.15 71.264 33.41 112.95 33.41z"/></svg>
</a>
<a target=_blank rel="noopener noreferrer" aria-label="share Setting up K8s and the vSphere Cloud Provider using kubeadm on reddit" href="https://reddit.com/submit?url=https%3a%2f%2fblah.cloud%2famp%2fkubernetes%2fsetting-up-k8s-and-the-vsphere-cloud-provider-using-kubeadm%2f&title=Setting%20up%20K8s%20and%20the%20vSphere%20Cloud%20Provider%20using%20kubeadm"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg>
</a>
<a target=_blank rel="noopener noreferrer" aria-label="share Setting up K8s and the vSphere Cloud Provider using kubeadm on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&url=https%3a%2f%2fblah.cloud%2famp%2fkubernetes%2fsetting-up-k8s-and-the-vsphere-cloud-provider-using-kubeadm%2f&title=Setting%20up%20K8s%20and%20the%20vSphere%20Cloud%20Provider%20using%20kubeadm&summary=Setting%20up%20K8s%20and%20the%20vSphere%20Cloud%20Provider%20using%20kubeadm&source=https%3a%2f%2fblah.cloud%2famp%2fkubernetes%2fsetting-up-k8s-and-the-vsphere-cloud-provider-using-kubeadm%2f"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg>
</a>
</div>
</section>
</footer><script src=https://utteranc.es/client.js repo=mylesagray/blog-comments issue-term=title theme=preferred-color-scheme crossorigin=anonymous async></script>
</article>
<aside class="hidden lg:block tableOfContentContainer" id=tableOfContentContainer>
<nav id=TableOfContents>
<ul>
<li><a href=#intro>Intro</a></li>
<li><a href=#prerequisites>Prerequisites</a>
<ul>
<li><a href=#tools>Tools</a></li>
<li><a href=#optional-use-of-tmux>Optional use of tmux</a></li>
</ul>
</li>
<li><a href=#setting-up-vms-with-k8s-components>Setting up VMs with K8s components</a>
<ul>
<li><a href=#on-all-nodes>On all nodes</a></li>
</ul>
</li>
<li><a href=#enabling-the-vmware-vsphere-cloud-provider>Enabling the VMware vSphere Cloud Provider</a>
<ul>
<li><a href=#on-the-masters>On the master(s)</a></li>
</ul>
</li>
<li><a href=#initialising-the-cluster-with-kubeadm>Initialising the cluster with kubeadm</a>
<ul>
<li><a href=#on-all-nodes-1>On all nodes</a></li>
<li><a href=#on-the-master-nodes>On the master node(s)</a></li>
<li><a href=#on-your-laptop>On your laptop</a></li>
<li><a href=#on-the-worker-nodes>On the worker nodes</a></li>
</ul>
</li>
<li><a href=#verify-setup>Verify setup</a></li>
</ul>
</nav>
</aside>
</div>
</main>
</div>
<footer class=footer>
<span>&copy; 2021 <a href=https://blah.cloud/>Blah, Cloud</a></span>
<span>
Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://git.io/hugopapermod rel=noopener target=_blank>PaperMod</a>
</span>
</footer>
<a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a>
<script>let menu=document.getElementById('menu');menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)},document.querySelectorAll('a[href^="#"]').forEach(a=>{a.addEventListener("click",function(b){b.preventDefault();var a=this.getAttribute("href").substr(1);window.matchMedia('(prefers-reduced-motion: reduce)').matches?document.querySelector(`[id='${decodeURIComponent(a)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(a)}']`).scrollIntoView({behavior:"smooth"}),a==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${a}`)})})</script>
<script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script>
<script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove('dark'),localStorage.setItem("pref-theme",'light')):(document.body.classList.add('dark'),localStorage.setItem("pref-theme",'dark'))})</script>
<script>document.querySelectorAll('pre > code').forEach(b=>{const c=b.parentNode.parentNode,a=document.createElement('button');a.classList.add('copy-code'),a.innerText='copy';function d(){a.innerText='copied!',setTimeout(()=>{a.innerText='copy'},2e3)}a.addEventListener('click',e=>{if('clipboard'in navigator){navigator.clipboard.writeText(b.textContent),d();return}const a=document.createRange();a.selectNodeContents(b);const c=window.getSelection();c.removeAllRanges(),c.addRange(a);try{document.execCommand('copy'),d()}catch(a){}c.removeRange(a)}),c.classList.contains("highlight")?c.appendChild(a):c.parentNode.firstChild==c||(b.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?b.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(a):b.parentNode.appendChild(a))})</script>
</body>
</html>